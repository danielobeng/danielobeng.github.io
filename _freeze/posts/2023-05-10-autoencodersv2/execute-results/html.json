{
  "hash": "34586d9c79c8a0a5fb23fcc1f074c728",
  "result": {
    "markdown": "---\ntitle: \"A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1\"\nauthor: \"Daniel\"\ndate: \"2023-05-10\"\ndescription: \"Part 1 in a series on Autoencoders, starting with the baics\"\ncategories: [tutorial, machine learning,examples]\ntags: [blog]\nexecute:\n    enabled: true\n    warning: false\n    echo: true\nformat:\n    html:\n        toc: true\n        toc-depth: 2\n        code-fold: true\n        code-tools: true\n        code-copy: false\njupyter: python3\n---\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.lines import Line2D\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n:::\n\n\n# you want to get as much out of every piece of information as possible\n\n## Motivation\n\nAutoencoders, while not novel in machine learning, have seen a significant upswing in recent years due to their powerful capabilities and adaptability. In the current landscape of advancing models like [diffusion models](https://en.wikipedia.org/wiki/Diffusion_model), comprehension of autoencoders has become a bedrock for those wanting to learn about the latest models.\n\nWelcome to the initial entry in our blog series on autoencoders, where we aim to translate complex concepts into simple, easy-to-understand explanations. We will largely avoid complex code and mathematical jargon in this post, focusing instead on nurturing your intuition around the core concepts that make autoencoders so fascinating. Subsequent entries will become more technical, building upon the ideas established here.\n\n### Understanding Autoencoders and Their Structure\n\nAutoencoders represent a distinct category of neural networks designed to condense input data and subsequently reconstruct it with high precision. This clever design includes two interconnected neural networks working collaboratively. Though the connection of two neural networks essentially forms a single network, viewing these as separate units is beneficial when first learning about their functionality.\n\nThe encoder and the decoder, nearly mirrored entities, make up these two neural networks. The encoder's task is to compress the input data into a compact form, while the decoder undertakes the decompression and reconstruction of this compressed data back to its original shape. A layer of information sitting between the encoder and decoder proves crucial to this process, a topic we'll explore further down the line.\n\n![Diagram of a basic Autoencoder (source: [Wikipedia](https://en.wikipedia.org/wiki/Autoencoder))](../media/images/Autoencoder_schema.png){#fig-autoencoder_schema width=\"450\"}\n\n### Applications of Autoencoders\n\nAutoencoders find utility in a wide range of applications such as:\n\n- Data Compression: The encoder of an autoencoder can compress an image file size, leading to faster sharing speeds for anyone with the relevant decoder. The effectiveness of this over traditional compression methods is largely dependent on the use case.\n\n- Image Reconstruction and Noise Reduction: From eliminating graininess and watermarks in images to advanced models for anomaly detection and generative modeling, autoencoders have found numerous applications.\n\n- Generative Modeling: Autoencoders play a critical role in generating new data, such as novel images or audio. An example can be seen in [Latent Diffusion Models](https://arxiv.org/pdf/2112.10752.pdf) that generate images based on text prompts or deepfakes where a person's face is generated by the model. [Variational Autoencoders](https://en.wikipedia.org/wiki/Variational_autoencoder), a more complex variant of autoencoders, are often employed in such advanced applications, something we will explore in future posts.\n\n:::{layout=\"[[1,1],[1, 1.5]]\" layout-valign=\"bottom\"}\n![Denoising Example - Original (top), original with noise(middle), de-noised reconstruction (bottom)](../media/images/denoising_example.png)\n\n![Watermark Removal Example - Original (top), original with watermark (middle), no watermark reconstruction (bottom)](../media/images/watermark_removal_example.png){height=\"250\"}\n\n![Diffusion Model Image Generation (source: [harvard.edu](https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/stable-diffusion-scratch))](../media/images/diffusion_proc1.gif)\n\n![Deepfake of Tom Cruise (created by the talented [Chris Ume](https://www.youtube.com/watch?v=wq-kmFCrF5Q))](../media/images/deepfake_tom_miles.gif)\n:::\n\n## Running Example setup\n\nThroughout this blog post, we will use a basic autoencoder model as an example. It compresses and reconstructs images from the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database).\n\nThe MNIST dataset (@fig-mnist_examples) consists of 28x28 pixel, black and white images of handwritten digits from 0 to 9. The pixel values range from 0-1 where 1=white, 0=black and any shade of grey will be a value between the two.\n\n![Images from the MNIST dataset](../media/images/mnist_examples.png){#fig-mnist_examples}\n\nThe goal is to use this example throughout the blog post as an anchor to which we can attach new concepts to as we learn them.\n\n@fig-mnist_reconstruction_example shows a handwritten digit that was reconstructed by the autoencoder model we will be using. The reconstruction is (purposefully) not perfect to highlight that it is a reconstruction.\n\n![MNIST image reconstruction](../media/images/linear_autoencoder_reconstruction.png){#fig-mnist_reconstruction_example}\n\n## Data compression: Retaining the Core Information\n\n### A compression analogy\n\nTo fully comprehend autoencoders, it is helpful to grasp the concept of data compression. The goal of a compression algorithm is to reduce the size of some given data.\n\nThe first clarification to make on compression is the following: compression of data means removing data. It is the only way to reduce the size of a file. The skill is in deciding what data to remove such that it is easy to add it back in when you decompress the data. \n\nThere are 2 important ideas to compression:\n\n1. The algorithms that encode and decode the data\n2. The form that the data takes when it is encoded\n\nHere's an example:\n\nImagine you are with a friend and you are tasked with manually copying strings of data from one piece of paper to another. The strings of data look like this:\n\n```\nAAARRBBBBMMMMMMMPPPEYYYYYBBBBBBBUUIIIIIIIII\nWWWAAAAAAAAAAATTTTNQQQQQQUJJNNNNNNRRRRRRRRR\nUUKLLLLLLLLFGHSSSSSSZZZZZZZZZZZZZZZZGGGGGGG\n...\netc.\n```\nThere are a lot of strings to copy so want to be efficient in this process. You decide for find a quicker way of passing the information to each other.\n\nYou assign different roles to each other: your friend will read the strings and you will write them down. As the reader, your friend decides to use an algorithm in order to shorten the amount of data they need to pass to you. We count the number of times a letter repeats and write down that number followed by the letter.\n\nFor example:\n```\nAAARRBBBBMMMMMMMPPPEYYYYYBBBBBBBUUIIIIIIIII\n```\nbecomes\n```\n3A2R4B7M3P1E5Y7B2U9I\n```\nWhat your friend is doing is \"encoding\" the strings of data. This particular encoding is compressing the data into a smaller form in a way that it can be perfectly reproduced - this is called \"lossless\" compression. Your friend is acting as an \"Encoder\".\n\nYour role is to \"decode\" the information back into it's original form. You are a \"Decoder\". All you need to do is know the algorithm your friend used to encode that data and do the reverse.\n\nWhile this is a straightforward example, why this works generalises to any situation. We can afford to reduce the amount of data because we found a more efficient way to express the same information with fewer bits (using only one number followed by one letter). There tend to be repeating sequences letters that don't need to be repeated.\n\ninsert image of tranfer of infor from data to algorithm?\n\nHow did we find this encoding-decoding technique? As humans, we have a lot of pre-existing knowledge and can spot simple patterns like this which we can use to design our algorithms. We can tell it would be more efficient to use a single letter and number\nIf the sequence had been single letter sequences (eg `AEJGOSKA`), we know that this method would have been less efficient as we would need twice as much information (`1A1E1J1G1O1S1K1A`).\n\nTo compress and decompress data, the algorithm must possess an adequate understanding of the rules of redundancy in the given data.\n\nIn most real-world scenarios, there is significantly more data involved, and the relationships are not as easily discernible by a human, leading to more complex algorithms with more intricate pre-existing knowledge. \n\nEarlier, we mentioned \"lossless\" compression. When the reconstruction is not perfect, we call this \"lossy\" compression. The `jpeg` file format for images is an example of a lossy compressed format. It makes the image size smaller at the detrement of losing some image quality.\n\nWhile this example used letters as the from of the data, given all things on computers are numbers, compression will work with anything; images, audio, video, text etc.\n\n### Autoencoder Compression with MNIST example\n\nAutoencoders work differently than general compression algorithms:\n\n- The algorithm is not designed by humans: autoencoders are neural networks, and the compression and decompression algorithm is developed by the network itself. While humans do have some input in determining hpw far to reduce the dimensionality of the data, the network must figure out how to make that work. This is where the \"auto\" in \"autoencoder\" comes from.\n- The algorithm is task-specific: autoencoders are trained on specific data and learn to compress data that is similar in nature. They can excel at compressing particular types of data, but they do not generalize beyond the data provided. If they are only fed images of cars, they won't compress images of flowers effectively. However, if given both images of cars and flowers, they could handle both.\n\nLet's look at the architechture of our image autoencoder (@fig-autoencoder_schema_example). Notice there are 2 parts: an encoder at the input that compresses data `X` into `h` and a decoder at the output that attempts to reconstruct `X` using only `h` to produce `X'`. \nBoth the encoder and decoder overlap in the middle - this middle layer is the the output of the encoder and contains the compressed form of the original image.\n\n![](../media/images/Autoencoder_schema_with_example_2d_latent.png){#fig-autoencoder_schema_example width=\"450\"}\n\nOur specific autoencoder has been designed to compress any 28x28 pixel image into 2 values (@fig-compressed_image_form). This means we are going from `28x28=784` values down to 2, so this is a significant reduction. Notice how we have a negative value for one of the pixels - these numbers longer represent pixel values in the same way.\n\n![Example compressed image to only 2 values](../media/images/2d_latent_compressed_form.png){#fig-compressed_image_form}\n\nWe say that the compressed form of the image is 2-dimensional.\n\n## Compression is dimensionality reduction\n\nWhat does it mean for the compressed representations to be lower-dimensional?\n\nAll this means is we reduce the number of categories (or features) of our data. In the case of an image, it means fewer pixel values. \n\nThe MNIST images (@fig-mnist_examples) are visualised in a 28x28 pixel grid. If we flatten out the image, we have `28x28=784` features, as each pixel is required to draw the digit.\n784 pixels can be thought of as the dimension size of the data. The first dimension represents the pixel value in the top left corner, 784th pixel represents the bottom right most pixel.\n\n@fig-orig_v_flattened shows a 50 pixel section of a flattened out MNIST image. The highlighted purple pixels are there to reference pixel position between the unflattened and flattend images.\n\nWhen the pixels are plotted, they are treated independently of each other. We go through each value, one by one and assign the correct colour to that value regardless of the values of other pixels. However, in any image, pixels are not independent of each other. We would not be able to compare two different hand written digits of a \"4\" and know they represented the same number. The only cases where the pixels are independent is in the case of random noise. This dependence between pixels is the property we exploit to reduce the dimensionality of the image.\n\n:::{layout=\"[[1.5,1]]\" layout-valign=\"bottom\" #fig-orig_v_flattened}\n![Alt text](../media/images/linear_autoencoder_reconstruction_2d_latent_orig.jpg)\n![Alt text](../media/images/linear_autoencoder_orig_2d_latent_flat.png){height=\"400\"}\n:::\n\nLet's examine the code that defines our autoencoder's architechture to understand how it reduces the dimensionality of the data. The code below uses the [Pytorch](https://pytorch.org) library to define the model layers.\n\nThe important lines of code involve the `Linear()` function (a [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)), which essentially takes us from a dimension of size $a$ to a dimension of size $b$ via `Linear(a,b)`.\n\nWe will not discuss the maths behind the `Linear()` function here as it is unnecessary for a first pass at understanding autoencoders.\n\n```python\n         self.encoder = torch.nn.Sequential(\n            Linear(28 * 28, 128),\n            # ReLU(),\n            Linear(128, 64),\n            # ReLU(),\n            Linear(64, 32),\n            # ReLU(),\n            Linear(32, 2),\n         )\n\n         self.decoder = torch.nn.Sequential(\n            Linear(2, 32),\n            # ReLU(),\n            Linear(32, 64),\n            # ReLU(),\n            Linear(64, 128),\n            # ReLU(),\n            Linear(128, 28 * 28),\n            # Sigmoid(),\n         )\n```\n\n\nThis architecture follows the same basic design as @fig-autoencoder_schema shows.\nThe encoder part of the model (`self.encoder`) reduces the data's dimension through these linear layers, little by little: `28*28 -> 128 -> 64 -> 32 -> 2`. The compressed representation ends up being 2 pixels wide (2-dimensional) after the final `Linear` function `Linear(32, 2)`. \n\nThe decoder (`self.decoder`) defines the same process as the encoder, but in reverse: linear layers go from smallest dimension (the middle layer) back to the original dimension of the image: 2 -> 32 -> 64 -> 128 -> 28*28. This is where the image reconstruction happens.\n\nHow is 2 pixels worth of information enough to reconstruct an image back to 784 pixels? \nFirst, the two values we are left with are not longer independent, meaning any combination of values means more than just \nSecond, the reconstruction is lossy, so we are allowing for errors in the reconstruction to allow for a smaller size compressed image.\nThird and most importantly, the \"prior experience\" contains a lot of the information needed for the reconstruction. It is held in the model's weights. The combination of the functions used to reduce the dimensionality of the data and their weights define a the new lower dimensional space - it is called the **latent space**. \n\n## Understanding Latent spaces\n\nIf we can reconstruct the data from a compressed representation, then the important information must still exist in that lower dimensional state.\n\nThe autoencoder must have learned something valuable about the \"world\" in which the data lives and stored that knowledge within the weights of the neural network that define the middle layer. That knowledge lies in what is known as the **latent space**.\n\nThe term \"latent\" refers to something that is present but hidden. In this context, compressed representations consisting of fewer dimensions already exist, it is just not obvious from the point of view of the higher dimensional space how to find them.\n\nConsider the plot of random points in @fig-3d_random_points. To describe any single point, we need 3 pieces of information - the x coordinate, the y coordinate, and the z coordinate. If the data is random, there is no correlation between x, y, and z. In other words, changing x does not affect y or z and vice versa. We say the data spans a 3-dimensional space. There are no latent spaces.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nnum_points = 100\nx = 6 * np.random.randn(num_points)\ny = 6 * np.random.randn(num_points)\nz = 6 * np.random.randn(num_points)\n\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d', facecolor='white')\nax.scatter(x, y, z, c='r', marker='o', alpha=0.3, s=5)\n\nax.set_xlabel('X', labelpad=-10)\nax.set_ylabel('Y', labelpad=-10)\nax.set_zlabel('Z', labelpad=-10)\n\nax.grid(False)\n\nax.set_xlim(-10, 10)\nax.set_ylim(-10, 10)\nax.set_zlim(-10, 10)\n\nax.xaxis._axinfo['juggled'] = (0, 0, 0)\nax.yaxis._axinfo['juggled'] = (1, 1, 1)\nax.zaxis._axinfo['juggled'] = (2, 2, 2)\n\nax.set_xticks([])\nax.set_yticks([])\nax.set_zticks([])\n\nax.plot([-10, 10], [0, 0], [0, 0], color='k', linewidth=1)\nax.plot([0, 0], [-10, 10], [0, 0], color='k', linewidth=1)\nax.plot([0, 0], [0, 0], [-10, 10], color='k', linewidth=1)\n\nax.xaxis.pane.fill = False\nax.yaxis.pane.fill = False\nax.zaxis.pane.fill = False\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![3D plot of random points](2023-05-10-autoencodersv2_files/figure-html/fig-3d_random_points-output-1.png){#fig-3d_random_points width=463 height=463 fig-align='center'}\n:::\n:::\n\n\nNow let's say x, y and z represent real world features: physical human traits.\n\n- `x = age`\n- `y = height`\n- `z = weight`\n\n The data would no longer be random. There are relationships between age,height, and weight. Additionally, some combinations of age, height and weight never occur in reality. You'll be hard-pressed to find a 300 kg 4-year-old measuring 50 cm tall. So, if we have good sources of data, the model will also tend to believe such combinations are not possible. There are underlying physiological rules defining the relationship. Those rules define a latent space.\n \nConversely, given two of these values, we can make an approximate guess at the third. If we can do that, then there likely exists a smaller dimensional space in which our data also resides, one that does not require all the features. If done correctly, it might only take 1 or 2 coordinates to describe any human by these traits.\n \nThe goal is to discover a structure that serves as our new coordinate system, allowing navigation with just two coordinates. Autoencoders, particularly the encoder portion, accomplish this by seeking a lower-dimensional latent space. The autoencoder's middle layer asserts the existence of a latent space of size $w$ and compels the network to find such a representation. Lacking this constraint, the middle layer would remain in its original high-dimensional space, merely copying the input and functioning as an identity transformation (multiplying everything by 1).\n\n @fig-height_weight_age demonstrates that the majority of points lie on or near a 2D latent space. By projecting these points onto the blue grid as close to their original positions as possible, we can create a 2-dimensional visualization. What the axes represent is no longer as simple as height, weight or age, but some combination of all of them. Due to this projection, the points won't precisely match their original locations, but they will be close. This process is analogous to the slightly blurred, imperfect image reconstruction from earlier on.\n\n::: {#fig-height_weight_age .cell freeze='true' layout='[[1,1]]' layout-valign='bottom' execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.decomposition import PCA\n\ndef generate_data(num_points):\n    age = np.random.randint(low=1, high=100, size=num_points)\n    height = np.zeros(num_points)\n    \n    for i, a in enumerate(age):\n        base_height = 50\n        growth_factor = 6\n        if a <= 20:\n            growth_rate = a / 4.0\n            height[i] = base_height + a * growth_factor  + np.random.normal(loc=0, scale=20)\n        else:\n            max_height = base_height + 20 * growth_factor\n            height[i] = max_height + np.random.normal(loc=0, scale=5)\n    \n    weight = height * 0.5 + 1.2 * np.random.normal(loc=0, scale=10, size=num_points)\n    data = np.column_stack((age, height, weight))\n    return data\n\ndata = generate_data(500)\n\npca = PCA(n_components=2)\nlatent_space = pca.fit_transform(data)\n\ngrid_size = 30\nx_grid, y_grid = np.meshgrid(np.linspace(latent_space[:, 0].min(), latent_space[:, 0].max(), grid_size),\n                             np.linspace(latent_space[:, 1].min(), latent_space[:, 1].max(), grid_size))\n\nlatent_grid = np.column_stack((x_grid.ravel(), y_grid.ravel()))\n\nprojected_grid = pca.inverse_transform(latent_grid)\nage_grid, height_grid, weight_grid = projected_grid[:, 0].reshape(grid_size, grid_size), projected_grid[:, 1].reshape(grid_size, grid_size), projected_grid[:, 2].reshape(grid_size, grid_size)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(data[:,0], data[:,1], data[:,2], c='r', marker='o', alpha=0.3, s=5, label='Original Data')\nax.plot_surface(age_grid, height_grid, weight_grid, alpha=0.2, color='blue', linewidth=0, antialiased=False, label='Latent Space')\n\nax.set_xlabel('Age', labelpad=-5)\nax.set_ylabel('Height', labelpad=-5)\nax.set_zlabel('Weight', labelpad=-10)\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_zticklabels([])\nplt.show()\n\nfig, ax = plt.subplots(figsize=(6,5))\nax.scatter(-latent_space[:, 0], -latent_space[:, 1], c='r', marker='o', alpha=0.6, s=10)\n\nx_min, x_max = latent_space[:, 0].min(), latent_space[:, 0].max()\ny_min, y_max = latent_space[:, 1].min(), latent_space[:, 1].max()\nmargin = 0.1\nx_range = x_max - x_min\ny_range = y_max - y_min\n\nx_grid, y_grid = np.meshgrid(np.linspace(x_min - margin * x_range, x_max + margin * x_range, 30),\n                             np.linspace(y_min - margin * y_range, y_max + margin * y_range, 30))\n\nfor i in range(x_grid.shape[0]):\n    ax.plot(-x_grid[i, :], -y_grid[i, :], c='blue', alpha=0.2, linewidth=0.5)\n    ax.plot(-x_grid[:, i], -y_grid[:, i], c='blue', alpha=0.2, linewidth=0.5)\n\nax.set_xlabel('Latent Space Axis 1')\nax.set_ylabel('Latent Space Axis 2')\nax.set_xticklabels([])\nax.set_yticklabels([])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Height vs Weight vs Age with Latent Space Overlay](2023-05-10-autoencodersv2_files/figure-html/fig-height_weight_age-output-1.png){#fig-height_weight_age-1 width=396 height=389}\n:::\n\n::: {.cell-output .cell-output-display}\n![Same data as (a) Projected onto Latent Space](2023-05-10-autoencodersv2_files/figure-html/fig-height_weight_age-output-2.png){#fig-height_weight_age-2 width=493 height=416}\n:::\n\nVisualisation of a Latent Space for height, weight and age in a population\n:::\n\n\nLatent spaces will not be so easy to visualise when it comes to complex data, and won't be a flat plane. This is why we might want the autoencoder to figure out the shape for us.\n\n## Dimensionality reduction reveals latent spaces\n\nWhen data is cleverly compressed, the reduced dimension space is a latent space and autoencoders can find these latent spaces for us.\n\nWhen an image is processed through our autoencoder model we can halt the process when it reaches the middle layer to obtain the 2-dimensional latent space representation of the image. Given there are only 2 dimensions, we end up with 2 coordinates.\n\n![Autoencoder Diagram with Decoder Removed](../media/images/Autoencoder_schema_with_example_2d_latent_sliced.png){width=\"450\" #fig-autoencoder_sliced}\n\nAfter running multiple images through the encoder, we can visualise the latent space by plotting the 2D coordinates of these images (@fig-tsne_mnist). This is the same idea as when we projected the age, height, width data onto a 2D latent space in  @fig-height_weight_age. It gives us an idea of what the latent space looks like, like sprinkling dust (i.e. the images) on an invisible landscape to see the topology.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.datasets as datasets\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nfrom sklearn.manifold import TSNE\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\ntrain_ds = torchvision.datasets.MNIST(\n    \"../data\",\n    train=True,\n    download=True,\n    transform=torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n        ]\n    ),\n)\n\ntest_ds = torchvision.datasets.MNIST(\n    \"../data\",\n    train=False,\n    download=True,\n    transform=torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n        ]\n    ),\n)\n\nbatch_size = 128\ntrain_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=False)\n\n\nclass AutoEncoder(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.encoder = torch.nn.Sequential(\n            torch.nn.Linear(28 * 28, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, 32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32, 2),\n        )\n\n        self.decoder = torch.nn.Sequential(\n            torch.nn.Linear(2, 32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 28 * 28),\n            torch.nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        enc = self.encoder(x)\n        dec = self.decoder(enc)\n        return dec\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = AutoEncoder()\nmodel_c = torch.compile(model)\nmodel_c.to(device)\nloss_f = torch.nn.MSELoss()\n\nopt = torch.optim.Adam(model.parameters(), lr=2e-3)\n\nepochs = 100\nout = []\nlosses = []\n\ntorch._dynamo.config.suppress_errors = True\n\n\ndef train_model():\n    model_c.train()\n\n    for epoch in tqdm(range(epochs)):\n        for img, _ in train_loader:\n            img = img.reshape(-1, 28 * 28).to(device)\n\n            reconstructed = model_c(img)\n            loss = loss_f(reconstructed, img)\n\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n            losses.append(loss.item())\n            print(f\"Loss: {loss.item()}\")\n        out.append((epoch, img, reconstructed))\n\n\nmodel_name = \"model_weights12-2dim.pt\"\nif Path(model_name).exists():\n    model.load_state_dict(torch.load(model_name))\nelse:\n    train_model()\n    torch.save(model.state_dict(), model_name)\n\nencoded_data = []\nlabels = []\noriginal_images = []\nc = 0\nfor i, (img, label) in tqdm(enumerate(test_loader)):\n    c += 1\n    img_flat = img.reshape(-1, 28 * 28)\n    out = model.encoder(img_flat)\n    encoded_data.extend(out.detach().cpu().numpy())\n    labels.extend(label.numpy())\n    original_images.extend(img_flat.cpu().numpy())\n    if c > 4:\n        break\n\nfig, ax = plt.subplots()\n\nimage_size = 28\npadding = 0.03\n\nencoded_data = np.array(encoded_data)\n\nx_min, x_max = np.min(encoded_data[:, 0]), np.max(encoded_data[:, 0])\ny_min, y_max = np.min(encoded_data[:, 1]), np.max(encoded_data[:, 1])\nx_range = x_max - x_min\ny_range = y_max - y_min\n\nshift_x = padding / 2\nshift_y = padding / 2\nman_shift = 0.1 \n\nfor idx, (x, y) in enumerate(encoded_data):\n    x_pos = (x - x_min) / x_range - shift_x - man_shift\n    y_pos = (y - y_min) / y_range - shift_y\n    image = original_images[idx].reshape(28, 28)\n    ab = plt.Axes(fig, [x_pos, y_pos, padding, padding], frame_on=False)\n    ab.set_axis_off()\n    fig.add_axes(ab)\n    ab.imshow(image, cmap=\"gray_r\")\n\nax.set_xticks([])\nax.set_yticks([])\n\nplt.show()\n```\n:::\n\n\n![2D Representation of latent dimension with original data projected onto it (source: Open code cell above for the code used to create this plot)](../media/images/mnist_2D_rep.png){#fig-tsne_mnist width=\"450\"}\n\n\nObserve how similar-looking digits cluster together, signifying that the model's middle layer can discern the resemblance between different digits.\n\n## Revealing latent space = autoencoder\n\nLatent space = middle layer of autoencoder, this is what we want to construct\n\nLet's create a new model with a higher dimensional latent space\nredo example with larger latent space to show better\n\nAn autoencoders' ability to learn essential features from the input data holds significant value. This knowledge becomes highly beneficial when applied to more sophisticated and engaging machine learning challenges.\n\nIn Part 2, we will delve into implementation of autoencoder-based models to put the concepts discussed here into a full example.\n\n",
    "supporting": [
      "2023-05-10-autoencodersv2_files"
    ],
    "filters": [],
    "includes": {}
  }
}