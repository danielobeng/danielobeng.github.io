{
  "hash": "97b9b81dc0515f22f094a62f692bda55",
  "result": {
    "markdown": "---\ntitle: \"A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1\"\nauthor: \"Daniel\"\ndate: \"2023-04-29\"\ndescription: \"Part 1 in a series on Autoencoders, starting with the baics\"\ncategories: [tutorial, machine learning,examples]\ntags: [blog]\nexecute:\n    enabled: true\n    warning: false\n    echo: true\nformat:\n    html:\n        toc: true\n        toc-depth: 2\n        code-fold: true\n        code-tools: true\n        code-copy: false\njupyter: python3\n---\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.lines import Line2D\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n:::\n\n\n# Autoencoder Intuition\n\nAutoencoders, though not a novel idea in the realm of machine learning, have gained significant attention in recent years due to their powerful capabilities and versatility. With the recent rise in popularity of [diffusion models](https://en.wikipedia.org/wiki/Diffusion_model), an understanding of autoencoders has become foundational knowledge to those wishing to learn about the latest models. \n\nWelcome to the first post in a blog series on autoencoders, where we will break down complex concepts into simple, digestible explanations. In this post, we will mostly steer clear of intricate code and mathematical jargon, focusing instead on building your intuition around the fundamental concepts that make autoencoders so fascinating. Later posts in the series will get more technical and build on the concepts described here.\n\nAlthough autoencoders can be utilized independently for tasks like image reconstruction and denoising (e.g., removing graininess or watermarks), their application extends to more sophisticated models as well. For instance, in [Latent Diffusion Models](https://arxiv.org/pdf/2112.10752.pdf) that generate images based on text prompts, autoencoders are component of the overall model. These advanced applications often employ a more complex variant called [Variational Autoencoders](https://en.wikipedia.org/wiki/Variational_autoencoder). **Deepfakes are also heavily based on autoencoder architechture** However, before delving into these advanced topics, it is worth investing time in understanding the fundamentals of basic autoencoders.\n\n:::{layout=\"[[1,1],[1, 1.5]]\" layout-valign=\"bottom\"}\n![Denoising Example - Original (top), original with noise(middle), de-noised reconstruction (bottom)](../media/images/denoising_example.png)\n\n![Watermark Removal Example - Original (top), original with watermark (middle), no watermark reconstruction (bottom)](../media/images/watermark_removal_example.png){height=\"250\"}\n\n![Diffusion Model Image Generation (source: [harvard.edu](https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/stable-diffusion-scratch))](../media/images/diffusion_proc1.gif)\n\n![Deepfake of Tom Cruise (created by the talented [Chris Ume](https://www.youtube.com/watch?v=wq-kmFCrF5Q))](../media/images/deepfake_tom_miles.gif)\n:::\n\n\n### Outline\n- The gist of an autoencoder\n    - Encoder and decoder components\n    - Compression and reconstruction of input data\n    - Why are Autoencoders useful?\n\n- Understanding compression\n    - Grid world example\n    - More context on compression\n    \n- Understanding Latent Spaces\n    - The concept of latent spaces and their importance in data representation\n    - Example using human traits as features\n    - The role of autoencoders in discovering latent spaces\n\n- Autoencoders and Dimensionality Reduction example with MNIST dataset\n    - Linear layer architechture explained with Pytorch\n    - Example outputs from an autoencoder\n    - Visualizing latent space produced by the autoencoder\n\n## The gist of an autoencoder\n\nAutoencoders stand out as a unique class of neural network designed to compress input data and subsequently reconstruct it with high accuracy. This ingenious architecture consists of two interconnected neural networks working in tandem. Ultimately, connecting two neural networks together creates a single neural network, but it is useful to think of these as distinct parts when first learning how they work.\n\nThe two neural networks are called the encoder and the decoder and are more or less mirror images of each other (@fig-autoencoder_schema). The encoder is responsible for compressing the input data into a more compact representation, while the decoder takes on the task of decompressing and reconstructing the compact representation back into the original form. Central to the success of this process is a layer of information nestled between the encoder and decoder. More on that later.\n\n![Diagram of a basic Autoencoder (source: [Wikipedia](https://en.wikipedia.org/wiki/Autoencoder))](../media/images/Autoencoder_schema.png){#fig-autoencoder_schema width=\"450\"}\n\n### From Images to Numerical Data: A Practical Example\n\nLet's consider an analogy to illustrate the concept of compression, which is what teh encoder does. Imagine you are tasked with describing the car in @fig-lambo to a friend, using only three words. Your friend must then draw and reproduce the car as accurately as possible based on your description.\n\n![An orange lambo (source: [Lamborghini](https://www.lamborghini.com/en-en))](../media/images/lambo.png){width=\"450\" #fig-lambo}\n\n\n\nYou might choose the words \"orange,\" \"fast,\" and \"angular\". In making these choices, you are drawing upon your experiences with other cars and how this particular car is different or similar to them. This allows you to generalize and create categories (also known as \"features\") based on aspects such as color, speed, and shape.\n\nIf you were allowed more words, your description would likely be more accurate. With 1000 words, you could describe a very reasonable representation. This is true, but only up to a point.\n\nBut do you really need 1000 words to be accurate with your description? Could you obtain nearly identical results with only 500 words? Possibly, depending on whether any of the 1000 words were correlated or redundant. You might have the words \"fast\" and \"200mph\" in your list which essentially mean the same thing - it won't help your friend draw the car any better having both.\n\nSo, how does this relate to autoencoders? Rather than words, we deal with numbers. In the context of images specifically, we deal with pixel values (a pixel colour is defined by 3 numbers for red, green and blue, e.g. pixel 1 = `(100, 200, 90)`), each of which can be though of as a feature. The goal of an autoencoder is to minimize redundancy these features. By doing so, it achieves compression, leaving only the most descriptive features that capture the essence of the input data.\n\n### Unleashing the Power of the Middle Layer\n\nNow, let's examine a practical example of how autoencoders compress and reconstruct data. Consider the original image in @fig-image_reconstruction, which we input into an autoencoder model. A compressed 15x2 representation is generated (as depicted in the middle column). This representation represents the middle layer of the model. Subsequently, the original image is reconstructed from this compressed form. Think about the journey of the image through the autoencoder in @fig-autoencoder_schema and when the transformations from original to middle column to reconstructed occur.\n\nIn this example, we have purposefully chosen a lower-quality reconstruction to emphasize that the process may not always be perfect, although it can achieve remarkable accuracy. It is indeed surprising how closely the reconstructed image resembles the original, despite using only a 15x2 representation as the starting point before decoding. At their core, images are simply numerical data on a computer, meaning this concept to be applied across various digital mediums, such as audio and text.\n\n![Image Reconstruction Example (source: [stackabuse.com](https://stackabuse.com/autoencoders-for-image-reconstruction-in-python-and-keras/))](../media/images/image_reconstruction.png){#fig-image_reconstruction}\n\nThe primary objective of an autoencoder extends beyond merely inputting an image and obtaining a reconstructed version. The true power of autoencoders lies in leveraging the information-rich middle layer in conjunction with a specific task, thereby enhancing the performance of machine learning models. Achieving accurate reconstruction of original data serves as an effective metric for monitoring how well autoencoder model has compressed information because the better it does this, the better the reconstruction. It helps in gauging the model's ability to capture and represent the essential features of the input data.\n\n### Why are Autoencoders useful?\n\nThe true value of autoencoders lies in how concisely they compress information by capturing only the most defining features of the data. Humans can describe a 1000 pixel image of a car using just three words because our brains already have an algorithm that selects relevant categories as the \"middle layer\". We aim to train autoencoders to achieve the same level of proficiency by forcing them to compress information, accomplished by feeding the model numerous images of cars until it learns the essential features.\n\nIdentifying these features unlocks many use cases, some of which include:\n\n- Data Compression: Utilize the encoder part of the autoencoder to reduce the size of an image file, enabling faster transfer speeds when shared with someone who possesses the corresponding decoder. Whether or not this is more useful than standard compression techniques really depends on the use case.\n- Noise Reduction: Improve image quality by eliminating graininess, as the model learns that graininess is not an essential aspect of the image.\n- Generative Modeling: Generate new information, such as novel images or audio, using generative models. Recently, [diffusion models](https://arxiv.org/pdf/1503.03585.pdf) have garnered significant attention in this area.\n- Anomaly Detection: Identify unusual patterns or outliers in the data.\n\nMoreover, the architecture of autoencoders is well-suited for semi-supervised or unsupervised learning, which simplifies the data acquisition process. We will explore this topic in greater detail in a future post.\n\n## Data compression: Retaining the Core Information\n\nTo fully comprehend autoencoders, it is helpful to grasp the concept of data compression.\n\nIn the context of computers, compression does not literally mean storing the same number of bits of information in a smaller space, like you would compress a gas into a smaller volume of space. Instead, it serves as an analogy for removing bits that are not considered important, thereby reducing the file size—reminiscent of autoencoders. The bits we remove are those we can reproduce using an algorithm.\n\nAlthough we don't compress data like we do the air in an oxygen tank, the end result is the same: we end up with something that takes up less space.\n\nLet's consider a simple example to illustrate this point. Imagine a 4x4 grid with four red squares in the center, surrounded by blue squares. Each square represents one bit of memory, totaling 16 bits of information. Our goal is to create an algorithm capable of compressing the size of this grid. We also know the following facts about blue and red squares:\n\n- Red squares always appear in 2x2 grids.\n- Blue squares consistently surround red square groups.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ngrid = np.ones((4, 4))\ngrid[1:3, 1:3] = 0\n\ncmap = plt.cm.get_cmap('coolwarm_r', 2)\n\nfig, ax = plt.subplots(figsize=(2, 2))\nax.imshow(grid, cmap=cmap, vmin=0, vmax=1)\n\nax.set_xticks(np.arange(0.5, 3.5, 1), minor=True)\nax.set_yticks(np.arange(0.5, 3.5, 1), minor=True)\n\nax.grid(which='minor', color='black', linewidth=2)\n\nax.set_xticks([])\nax.set_yticks([])\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](2023-04-17-autoencoders_files/figure-html/cell-3-output-1.png){width=182 height=182 fig-align='center'}\n:::\n:::\n\n\nWith this knowledge, we can develop an algorithm to compress the information in the image above. We can represent the original image using just one red square and one blue square side by side, as we know that a red square must be accompanied by three more red squares in a 2x2 formation. If a blue square is adjacent to the red square, it must surround the red squares. We have effectively compressed the information from 16 bits to 2 bits, retaining only the core essence of this blue and red square world.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfaint_red = (0.705673158, 0.01555616, 0.150232812, 0.3)\nfaint_blue = (0.2298057, 0.298717966, 0.753683153, 0.3)\nred = (0.705673158, 0.01555616, 0.150232812)\nblue = (0.2298057, 0.298717966, 0.753683153)\n\ncustom_cmap = ListedColormap([faint_red, faint_blue, red, blue])\n\ngrid = np.zeros((4, 4))\ngrid[0, ::1] = 1\ngrid[3, ::1] = 1\ngrid[:, 0] = 1\ngrid[:, 3] = 1\n\ngrid[2, 2] = 2\ngrid[2, 3] = 3\n\nfig, ax = plt.subplots(figsize=(2, 2))\nax.imshow(grid, cmap=custom_cmap, vmin=0, vmax=3)\nax.set_xticks(np.arange(0.5, 3.5, 1), minor=True)\nax.set_yticks(np.arange(0.5, 3.5, 1), minor=True)\n\nax.grid(which='minor', color='lightgray', linewidth=2)\n\nax.add_line(Line2D([1.5, 1.5], [1.5, 2.5], color='black', linewidth=2))\nax.add_line(Line2D([2.5, 2.5], [1.5, 2.5], color='black', linewidth=2))\nax.add_line(Line2D([3.5, 3.5], [1.5, 2.5], color='black', linewidth=2))\nax.add_line(Line2D([1.5, 3.5], [1.5, 1.5], color='black', linewidth=2))\nax.add_line(Line2D([1.5, 3.5], [2.5, 2.5], color='black', linewidth=2))\nax.set_xticks([])\nax.set_yticks([])\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](2023-04-17-autoencoders_files/figure-html/cell-4-output-1.png){width=182 height=182 fig-align='center'}\n:::\n:::\n\n\nIt is worth noting the role of the algorithm in this process. To compress and decompress the data, the algorithm must possess an adequate understanding of this tiny world. In our example, we designed the algorithm with pre-existing knowledge.\n\nIn most real-world scenarios, there is significantly more data involved, and the relationships are not as easily discernible, leading to more complex algorithms with more intricate pre-existing knowledge.\n\n### More context on compression\nWhen dealing with more complex data, such as an image containing a greater number of bits, the design of the algorithm must be more sophisticated. However, the main principle remains the same: devise ways to represent a certain number of bits with fewer bits, which necessitates understanding the essential relationships between those bits of information.\n\nNot all compression algorithms perform equally. Some algorithms allow you to perfectly replicate the original information, referred to as \"lossless\" compression. Conversely, when the information cannot be replicated flawlessly, it is called \"lossy\" compression. The `jpeg` image format, for instance, employs a lossy algorithm. `jpeg` files are small due to an algorithm working behind the scenes. In general, lossy algorithms can compress more data than lossless ones.\n\nYour choice between the two depends on your specific needs. For an image, a lossy compression may suffice if you only need it to be clear enough to read a document. However, an audiophile seeking a perfect reproduction of their favorite song would likely opt for lossless compression.\n\n### Back to Autoencoders\nWhat sets autoencoders apart from general compression algorithms?\n\n- The algorithm is not designed by humans: autoencoders are neural networks, and the compression and decompression algorithm is developed by the network itself. While humans do have some input in determining the size of the middle layer, for example, the network must figure out how to make it work. This is where the \"auto\" in \"autoencoder\" comes from.\n- The algorithm is task-specific: autoencoders are trained on specific data and learn to compress data that is similar in nature. They can excel at compressing particular types of data, but they do not generalize beyond the data provided. If they are only fed images of cars, they won't compress images of flowers effectively. However, if given both images of cars and flowers, they could handle both.\nFor successful compression, the network must have learned something valuable about the data's world and stored that knowledge in the middle layer of the network. That knowledge lies in what is known as the **latent space**.\n\nThe term \"latent\" refers to something that is present but hidden. In this context, compressed representations consisting of fewer dimensions already exist, it is just not obvious from the point of view of the higher dimensional space how to find them.\n\n## Understanding Latent Spaces\n\nConsider the plot of random points in @fig-3d_random_points. To describe any single point, we need 3 pieces of information - the x coordinate, the y coordinate, and the z coordinate. If the data is random, there is no correlation between x, y, and z. In other words, changing x does not affect y or z and vice versa. We say the data spans a 3-dimensional space. There are no latent spaces.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nnum_points = 100\nx = 6 * np.random.randn(num_points)\ny = 6 * np.random.randn(num_points)\nz = 6 * np.random.randn(num_points)\n\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d', facecolor='white')\nax.scatter(x, y, z, c='r', marker='o', alpha=0.3, s=5)\n\nax.set_xlabel('X', labelpad=-10)\nax.set_ylabel('Y', labelpad=-10)\nax.set_zlabel('Z', labelpad=-10)\n\nax.grid(False)\n\nax.set_xlim(-10, 10)\nax.set_ylim(-10, 10)\nax.set_zlim(-10, 10)\n\nax.xaxis._axinfo['juggled'] = (0, 0, 0)\nax.yaxis._axinfo['juggled'] = (1, 1, 1)\nax.zaxis._axinfo['juggled'] = (2, 2, 2)\n\nax.set_xticks([])\nax.set_yticks([])\nax.set_zticks([])\n\nax.plot([-10, 10], [0, 0], [0, 0], color='k', linewidth=1)\nax.plot([0, 0], [-10, 10], [0, 0], color='k', linewidth=1)\nax.plot([0, 0], [0, 0], [-10, 10], color='k', linewidth=1)\n\nax.xaxis.pane.fill = False\nax.yaxis.pane.fill = False\nax.zaxis.pane.fill = False\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![3D plot of random points](2023-04-17-autoencoders_files/figure-html/fig-3d_random_points-output-1.png){#fig-3d_random_points width=463 height=463 fig-align='center'}\n:::\n:::\n\n\nNow let's say x, y and z represent real world features: physical human traits.\n\n- `x = age`\n- `y = height`\n- `z = weight`\n\n The data would no longer be random. There are relationships between age,height, and weight. Additionally, some combinations of age, height and weight never occur in reality. You'll be hard-pressed to find a 300 kg 4-year-old measuring 50 cm tall. So, if we have good sources of data, the model will also tend to believe such combinations are not possible. There are underlying physiological rules defining the relationship. Those rules define a latent space.\n \nConversely, given two of these values, we can make an approximate guess at the third. If we can do that, then there likely exists a smaller dimensional space in which our data also resides, one that does not require all the features. If done correctly, it might only take 1 or 2 coordinates to describe any human by these traits.\n \nThe goal is to discover a structure that serves as our new coordinate system, allowing navigation with just two coordinates. Autoencoders, particularly the encoder portion, accomplish this by seeking a lower-dimensional latent space. The autoencoder's middle layer asserts the existence of a latent space of size $w$ and compels the network to find such a representation. Lacking this constraint, the middle layer would remain in its original high-dimensional space, merely copying the input and functioning as an identity transformation (multiplying everything by 1).\n\n @fig-height_weight_age demonstrates that the majority of points lie on or near a 2D latent space. By projecting these points onto the blue grid as close to their original positions as possible, we can create a 2-dimensional visualization. What the axes represent is no longer as simple as height, weight or age, but some combination of all of them. Due to this projection, the points won't precisely match their original locations, but they will be close. This process is analogous to the slightly blurred, imperfect image reconstruction from earlier on.\n\n::: {#fig-height_weight_age .cell freeze='true' layout='[[1,1]]' layout-valign='bottom' execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.decomposition import PCA\n\ndef generate_data(num_points):\n    age = np.random.randint(low=1, high=100, size=num_points)\n    height = np.zeros(num_points)\n    \n    for i, a in enumerate(age):\n        base_height = 50\n        growth_factor = 6\n        if a <= 20:\n            growth_rate = a / 4.0\n            height[i] = base_height + a * growth_factor  + np.random.normal(loc=0, scale=20)\n        else:\n            max_height = base_height + 20 * growth_factor\n            height[i] = max_height + np.random.normal(loc=0, scale=5)\n    \n    weight = height * 0.5 + 1.2 * np.random.normal(loc=0, scale=10, size=num_points)\n    data = np.column_stack((age, height, weight))\n    return data\n\ndata = generate_data(500)\n\npca = PCA(n_components=2)\nlatent_space = pca.fit_transform(data)\n\ngrid_size = 30\nx_grid, y_grid = np.meshgrid(np.linspace(latent_space[:, 0].min(), latent_space[:, 0].max(), grid_size),\n                             np.linspace(latent_space[:, 1].min(), latent_space[:, 1].max(), grid_size))\n\nlatent_grid = np.column_stack((x_grid.ravel(), y_grid.ravel()))\n\nprojected_grid = pca.inverse_transform(latent_grid)\nage_grid, height_grid, weight_grid = projected_grid[:, 0].reshape(grid_size, grid_size), projected_grid[:, 1].reshape(grid_size, grid_size), projected_grid[:, 2].reshape(grid_size, grid_size)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(data[:,0], data[:,1], data[:,2], c='r', marker='o', alpha=0.3, s=5, label='Original Data')\nax.plot_surface(age_grid, height_grid, weight_grid, alpha=0.2, color='blue', linewidth=0, antialiased=False, label='Latent Space')\n\nax.set_xlabel('Age', labelpad=-5)\nax.set_ylabel('Height', labelpad=-5)\nax.set_zlabel('Weight', labelpad=-10)\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_zticklabels([])\nplt.show()\n\nfig, ax = plt.subplots(figsize=(6,5))\nax.scatter(-latent_space[:, 0], -latent_space[:, 1], c='r', marker='o', alpha=0.6, s=10)\n\nx_min, x_max = latent_space[:, 0].min(), latent_space[:, 0].max()\ny_min, y_max = latent_space[:, 1].min(), latent_space[:, 1].max()\nmargin = 0.1\nx_range = x_max - x_min\ny_range = y_max - y_min\n\nx_grid, y_grid = np.meshgrid(np.linspace(x_min - margin * x_range, x_max + margin * x_range, 30),\n                             np.linspace(y_min - margin * y_range, y_max + margin * y_range, 30))\n\nfor i in range(x_grid.shape[0]):\n    ax.plot(-x_grid[i, :], -y_grid[i, :], c='blue', alpha=0.2, linewidth=0.5)\n    ax.plot(-x_grid[:, i], -y_grid[:, i], c='blue', alpha=0.2, linewidth=0.5)\n\nax.set_xlabel('Latent Space Axis 1')\nax.set_ylabel('Latent Space Axis 2')\nax.set_xticklabels([])\nax.set_yticklabels([])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Height vs Weight vs Age with Latent Space Overlay](2023-04-17-autoencoders_files/figure-html/fig-height_weight_age-output-1.png){#fig-height_weight_age-1 width=397 height=389}\n:::\n\n::: {.cell-output .cell-output-display}\n![Same data as (a) Projected onto Latent Space](2023-04-17-autoencoders_files/figure-html/fig-height_weight_age-output-2.png){#fig-height_weight_age-2 width=493 height=416}\n:::\n\nVisualisation of a Latent Space for height, weight and age in a population\n:::\n\n\nLatent spaces will not be so easy to visualise when it comes to complex data, and won't be a flat plane. This is why we want the Autoencoder to figure out the shape for us.\n\n## Autoencoder for compression example - MNIST\nIn this section, we will cover a high level the fundamental concepts of how Autoencoders compress and reconstruct data using the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) as an example. A more in-depth training guide will be provided in a future post.\n\nThe MNIST dataset (@fig-mnist_examples) consists of 28x28 pixel images of handwritten digits from 0 to 9, resulting in `28x28=784` features (each pixel is a feature) to describe a digit.\n\nWe will explore the following aspects of the model:\n\n- Constructing the autoencoder model - an overview of the compression process.\n- The autoencoder architechture - how the model reproduces the input by passing it through the input, middle layer, and output. \n- Slicing the model by removing the decoder section, which allows us to obtain the latent space coordinates of an input image instead of its reconstruction.\n- Visualizing the position of images in the latent space\n\n![Images from the MNIST dataset](../media/images/mnist_examples.png){#fig-mnist_examples}\n\n\nExamining the code that defines the autoencoder's architecture using the MNIST data helps us understand how it reduces the dimensionality of the data. The code below uses the [Pytorch](https://pytorch.org) library to define the model layers.\n\nThe important lines of code involve the `Linear()` function (a [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)), which essentially takes us from a dimension of size $a$ to a dimension of size $b$ via `Linear(a,b)`.\n\nThe encoder part of the model (`self.encoder`) reduces the data's dimension through these linear layers, little by little: `28*28 -> 128 -> 64 -> 32 -> 10`.\n\nThe decoder (`self.decoder`) is the same process, but in reverse: linear layers go from smallest dimension (the middle layer, consiting of 10 dimensions) to the original dimension of the image: 10 -> 32 -> 64 -> 128 -> 28*28. This is where the reconstruction happens.\n\n```python\n         self.encoder = torch.nn.Sequential(\n            Linear(28 * 28, 128),\n            # ReLU(),\n            Linear(128, 64),\n            # ReLU(),\n            Linear(64, 32),\n            # ReLU(),\n            Linear(32, 10),\n         )\n\n         self.decoder = torch.nn.Sequential(\n            Linear(10, 32),\n            # ReLU(),\n            Linear(32, 64),\n            # ReLU(),\n            Linear(64, 128),\n            # ReLU(),\n            Linear(128, 28 * 28),\n            # Sigmoid(),\n         )\n```\n\nThis very autoencoder architecture was used to create the reconstruction in @fig-autoencoder_example. Again, the reconstructed image is intentionally blurred to better illustrate the concept of reconstruction. In practice, with such small images, an autoencoder can achieve reconstructions that more closely resemble the original input.\n\n![Autoencoder example](../media/images/Autoencoder_schema_with_example.png){#fig-autoencoder_example width=\"450\"}\n\n![Input Image and Decoder Output](../media/images/linear_autoencoder_reconstruction.png){#fig-lin_autoencoder_reconstruction width=\"450\"}\n\nWhen an image is processed through this model, it generates a reconstructed output. However, if we halt the process at the middle layer, we obtain the 10-dimensional latent space representation, which consists of a set of 10 coordinates.\n\n![Autoencoder Diagram with Decoder Removed](../media/images/Autoencoder_schema_sliced.png){width=\"450\" #fig-autoencoder_sliced}\n\n![Output from Autoencoder Middle Layer](../media/images/middle_layer.png){height=\"250\" #fig-middle_layer}\n\nMoreover, we can visualise the latent space by plotting the positions of multiple images that were passed through the encoder layer only in (@fig-tsne_mnist). In order to interpret what a 10-dimensional space looks like, we need to approximate it by further flattening it down to 2 dimensions using a technique called t-SNE (how this works is not important to understand).\n\nObserve how similar-looking digits cluster together, signifying that the model's middle layer can discern the resemblance between different digits. Most notably, we can now characterize a particular digit, such as \"1,\" using only two coordinates instead of the original 784 features, highlighting the power of autoencoders for dimensionality reduction.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nfrom sklearn.manifold import TSNE\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom tqdm import tqdm\n\ntrain_ds = torchvision.datasets.MNIST(\n    \"../data\",\n    train=True,\n    download=True,\n    transform=torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n        ]\n    ),\n)\n\ntest_ds = torchvision.datasets.MNIST(\n    \"../data\",\n    train=False,\n    download=True,\n    transform=torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n        ]\n    ),\n)\n\nepochs = 20\nbatch_size = 128\ntrain_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=False)\n\n\nclass AutoEncoder(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.encoder = torch.nn.Sequential(\n            torch.nn.Linear(28 * 28, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, 32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32, 10),\n        )\n\n        self.decoder = torch.nn.Sequential(\n            torch.nn.Linear(10, 32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 28 * 28),\n            torch.nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        enc = self.encoder(x)\n        dec = self.decoder(enc)\n        return dec\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = AutoEncoder()\n# model = torch.compile(model) #if using torch 2.0\n\nmodel.to(device)\nloss_f = torch.nn.MSELoss()\n\nopt = torch.optim.Adam(model.parameters(), lr=2e-3)\n\nout = []\nlosses = []\n\n\ndef train_model():\n    model.train()\n\n    for epoch in tqdm(range(epochs)):\n        for img, _ in train_loader:\n            img = img.reshape(-1, 28 * 28).to(device)\n\n            reconstructed = model(img)\n            loss = loss_f(reconstructed, img)\n\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n            losses.append(loss.item())\n            print(f\"Loss: {loss.item()}\")\n        out.append((epoch, img, reconstructed))\n\n\nmodel_name = \"model_weights.pt\"\nif Path(model_name).exists():\n    model.load_state_dict(torch.load(model_name))\nelse:\n    train_model()\n    torch.save(model.state_dict(), model_name)\n\ntest_image, _ = test_ds[4]\n\nencoded_data = []\noriginal_images = []\nc = 0\nfor i, (img, label) in tqdm(enumerate(test_loader)):\n    c += 1\n    img_flat = img.reshape(-1, 28 * 28)\n    out = model.encoder(img_flat)\n    encoded_data.extend(out.detach().cpu().numpy())\n    original_images.extend(img_flat.cpu().numpy())\n    if c > 3:\n        break\n\ntsne = TSNE(n_components=2, random_state=42)\ntsne_output = tsne.fit_transform(np.array(encoded_data))\n\nfig, ax = plt.subplots()\n\nimage_size = 28\npadding = 0.06\npadding_in_pixels = int(padding * image_size)\n\ntsne_min, tsne_max = np.min(tsne_output, axis=0), np.max(tsne_output, axis=0)\ntsne_range = tsne_max - tsne_min\nnormalized_output = (tsne_output - tsne_min) / tsne_range\n\nfor idx, (x, y) in tqdm(enumerate(normalized_output)):\n    x_pos = x * (1 - 2 * padding) + padding\n    y_pos = y * (1 - 2 * padding) + padding\n    image = original_images[idx].reshape(28, 28)\n    im = np.array(image, dtype=np.float32)\n    ab = plt.Axes(fig, [x_pos, y_pos, padding, padding])\n    ab.set_axis_off()\n    fig.add_axes(ab)\n    ab.imshow(im, cmap=\"gray_r\")\n\nax.set_xticks([])\nax.set_yticks([])\n\nplt.show()\n```\n:::\n\n\n![2D Representation of latent dimension (source: Open code cell above for the code used to create this plot)](../media/images/tsne_mnist.png){#fig-tsne_mnist width=\"450\"}\n\nAn autoencoders' ability to learn essential features from the input data holds significant value. This knowledge becomes highly beneficial when applied to more sophisticated and engaging machine learning challenges.\n\nIn Part 2, we will delve into implementation of autoencoder-based models to put the concepts discussed here into a full example.\n\n",
    "supporting": [
      "2023-04-17-autoencoders_files"
    ],
    "filters": [],
    "includes": {}
  }
}