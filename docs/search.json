[
  {
    "objectID": "posts/2023-05-10-autoencodersv2.html",
    "href": "posts/2023-05-10-autoencodersv2.html",
    "title": "A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.lines import Line2D\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "posts/2023-05-10-autoencodersv2.html#motivation",
    "href": "posts/2023-05-10-autoencodersv2.html#motivation",
    "title": "A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1",
    "section": "Motivation",
    "text": "Motivation\nAutoencoders, while not novel in machine learning, have seen a significant upswing in recent years due to their powerful capabilities and adaptability. In the current landscape of advancing models like diffusion models, comprehension of autoencoders has become a bedrock for those wanting to learn about the latest models.\nWelcome to the initial entry in a blog series on autoencoders, where the aim is to translate complex concepts into simple, easy-to-understand explanations. We will largely avoid complex code and mathematical jargon in this post, focusing instead on nurturing your intuition around the core concepts that make autoencoders so fascinating. Subsequent entries will become more technical, building upon the ideas established here.\n\nAutoencoders Structure\nAutoencoders represent a distinct category of neural networks, designed to condense input data and subsequently reconstruct it with high accuracy. This clever design includes two interconnected networks working collaboratively. Though the connection of two networks essentially forms a single network, viewing these as separate units is beneficial when first learning about their functionality.\nThe encoder and the decoder, nearly mirrored entities, make up these two units. The encoder’s task is to compress the input data into a compact form, while the decoder undertakes the decompression and reconstruction of this compressed data back to its original shape. The central layer sitting between the encoder and decoder will be an important aspect of the network to understand.\n\n\n\nFigure 1: Diagram of a basic Autoencoder (source: Wikipedia)\n\n\n\n\nApplications of Autoencoders\nAutoencoders find utility in a wide range of applications such as:\n\nData Compression: The encoder of an autoencoder can compress an image file size, leading to faster sharing speeds for anyone with the relevant decoder. The effectiveness of this over traditional compression methods is largely dependent on the use case.\nImage Reconstruction and Noise Reduction: From eliminating graininess or watermarks in images to advanced models for anomaly detection and generative modeling, autoencoders have found numerous applications.\nGenerative Modeling: Autoencoders play a critical role in generating new data, such as novel images or audio. An example can be seen in Latent Diffusion Models that generate images based on text prompts or deepfakes where a person’s face is generated by the model. Variational Autoencoders, a more complex variant of autoencoders, are often employed in such advanced applications, something we will explore in future posts.\n\n\n\n\n\n\n\n\nDenoising Example - Original (top), original with noise(middle), de-noised reconstruction (bottom)\n\n\n\n\n\n\n\nWatermark Removal Example - Original (top), original with watermark (middle), no watermark reconstruction (bottom)\n\n\n\n\n\n\n\n\n\nDiffusion Model Image Generation (source: harvard.edu)\n\n\n\n\n\n\n\nDeepfake of Tom Cruise (created by the talented Chris Ume)\n\n\n\n\nFigure 2: Example use cases of autoencoders"
  },
  {
    "objectID": "posts/2023-05-10-autoencodersv2.html#running-example-setup",
    "href": "posts/2023-05-10-autoencodersv2.html#running-example-setup",
    "title": "A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1",
    "section": "Running Example setup",
    "text": "Running Example setup\nThroughout this blog post, we will use a basic autoencoder model as an example. It compresses and reconstructs images from the MNIST dataset.\nThe MNIST dataset (Figure 2) consists of 28x28 pixel, black and white images of handwritten digits from 0 to 9. The pixel values range from 0-1 where 1=white, 0=black and any shade of grey will be a value between the two.\n\n\n\nFigure 2: Images from the MNIST dataset\n\n\nThe goal is to use this example throughout the blog post as an anchor to which we can attach new concepts to as we learn them.\nFigure 3 shows a handwritten digit that was reconstructed by the autoencoder model we will be using. The reconstruction is (purposefully) not perfect to highlight that it is a reconstruction.\n\n\n\nFigure 3: MNIST image reconstruction"
  },
  {
    "objectID": "posts/2023-05-10-autoencodersv2.html#data-compression-retaining-the-core-information",
    "href": "posts/2023-05-10-autoencodersv2.html#data-compression-retaining-the-core-information",
    "title": "A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1",
    "section": "Data compression: Retaining the Core Information",
    "text": "Data compression: Retaining the Core Information\nTo fully comprehend autoencoders, it is helpful to grasp the concept of data compression. The goal of a compression algorithm is to reduce the size of data with minimal loss of information.\nThe first clarification to make on compression is the following: compression of data means removing data. It is the only way to reduce the size of a file. The skill is in deciding what data to remove such that it is easy to add it back in when you decompress the data.\n\nA compression analogy\nImagine you are with a friend and you are tasked with manually copying strings of data from one piece of paper to another. The strings of data look like this:\nAAARRBBBBMMMMMMMPPPEYYYYYBBBBBBBUUIIIIIIIII\nWWWAAAAAAAAAAATTTTNQQQQQQUJJNNNNNNRRRRRRRRR\nUUKLLLLLLLLFGHSSSSSSZZZZZZZZZZZZZZZZGGGGGGG\n...\netc.\nThere are a lot of strings to copy so you want to be efficient in this process. You decide for find a quicker way of passing the information to each other.\nYou assign yourselves different roles: your friend will read the strings and you will write them down. As the reader, your friend decides to use an algorithm in order to shorten the amount of data they need to pass to you. They count the number of times each letter repeats and pass you that number along with the corresponding letter.\nFor example:\nAAARRBBBBMMMMMMMPPPEYYYYYBBBBBBBUUIIIIIIIII\nbecomes\n3A2R4B7M3P1E5Y7B2U9I\nWhat your friend is doing is “encoding” the strings of data. This particular encoding is compressing the data into a smaller form in a way that it can be perfectly reproduced - this is called “lossless” compression. Your friend is acting as an “Encoder” (sidenote: when a reconstruction is not perfect, we call this “lossy” compression. The jpeg file format for images is an example of a lossy compressed format. It makes the image size smaller but loses some image quality).\nYour role is to “decode” the information back into it’s original form. You are a “Decoder”. All you need to do is know the algorithm your friend used to encode that data and do the reverse.\nWhile this is a straightforward example, why this works generalises to any situation. We can afford to reduce the amount of bits of data because we found a more efficient way to express the same information with fewer bits (using only one number followed by one letter per repeating sequence). The observation we made was that letters tend to repeat themselves, and we can take advantage of that.\nTo compress and decompress data, the algorithm must possess an adequate understanding of the rules of redundancy in the given data. As humans, we have a lot of pre-existing knowledge and can spot simple patterns this which we can use to design our algorithms. We can tell it would be more efficient to use a single letter and number. If the sequence had been single letter sequences (eg AEJGOSKA), we know that this method would have been less efficient as we would need twice as much information (1A1E1J1G1O1S1K1A).\nThis example used letters for the data type but given all things on computers are numbers, compression will work with anything; images, audio, video, text etc.\nIn most real-world scenarios, there is significantly more data involved, and the relationships are not as easily discernible by a human, leading to more complex algorithms with more intricate pre-existing knowledge. We either need a smart human or a computer to figure out the algorithm for us (this is where autoencoders come in).\n\n\nAutoencoder Compression with MNIST example\nAutoencoders work differently than general compression algorithms:\n\nThe algorithm is not designed by humans: autoencoders are neural networks, and the compression and decompression algorithm is developed by the network itself. While humans do have some input in determining far to compress the data, the autoencoder must figure out how to make that work. This is where the “auto” in “autoencoder” comes from.\nThe algorithm is task-specific: autoencoders are trained on specific data and learn to compress data that is similar in nature. They can excel at compressing particular types of data, but they do not generalize beyond the data provided. If they are only fed images of cars, they won’t compress images of flowers effectively. However, if given both images of cars and flowers, they could handle both.\n\nLet’s have another look at the architechture of our image autoencoder (Figure 5). Notice there are 2 parts: an encoder at the input that compresses data X (the image) into h, the middle layer (a compressed form of X), and a decoder at the output that attempts to reconstruct X using only h to produce X'.\n\n\n\nFigure 5: Autoencoder Schema with example data\n\n\nOur specific autoencoder has been designed to compress any 28x28 pixel image into 2 values (Figure 6). This means we are going from 28x28=784 values down to 2, so this is a significant reduction. Notice how we have a negative value for one of the pixels - these numbers longer represent pixel values in the same way. We say that the compressed form of the image is 2-dimensional.\n\n\n\nFigure 6: Example compressed image to only 2 values"
  },
  {
    "objectID": "posts/2023-05-10-autoencodersv2.html#compression-is-dimensionality-reduction",
    "href": "posts/2023-05-10-autoencodersv2.html#compression-is-dimensionality-reduction",
    "title": "A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1",
    "section": "Compression is dimensionality reduction",
    "text": "Compression is dimensionality reduction\nAutoencoders have uses beyond just compression, but compression is integral to any autoencoder task. In essence, compression functions as a form of dimensionality reduction, simplifying the data by decreasing the number of its features or categories.\nTo explain further, dimensionality reduction simply means that we decrease the number of categories (or features) in our data. For instance, when dealing with an image, this corresponds to a reduction in pixel values. Consider the MNIST images, which are visualized on a grid of 28x28 pixels (Figure 3). When we “flatten” the image, we end up with 28x28=784 features, each pixel being a feature required to compose the digit.\nThis number of pixels – 784 – can be thought of as the dimension size of the data. Here, the first dimension corresponds to the pixel value in the top left corner, while the 784th pixel represents the bottom rightmost pixel. Figure 7 provides a visual representation of a 50-pixel section from a “flattened” MNIST image, with purple pixels highlighting the corresponding positions between the unflattened and flattened images.\nWhen pixels are rendered as an image, they are treated independently; we assign color to each pixel based on its value, irrespective of other pixels. However, the pixels within an image are not truly independent - if they were, we wouldn’t recognize the image. Two hand-drawn “4”s, despite their differences, can still be recognized as the same number. This interdependence of pixels, seen in all but random noise, is precisely the property we leverage to reduce an image’s dimensionality.\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 7: (a) Original MNIST image (b) 50px section of flattened MNIST image\n\n\nLet’s examine the code that defines our autoencoder’s architechture to understand how it reduces the dimensionality of the data. The code below uses the Pytorch library to define the model layers.\nThe important lines of code involve the Linear() function (a linear layer), which essentially takes us from a dimension of size \\(a\\) to a dimension of size \\(b\\) via Linear(a,b).\nWe will not discuss the maths behind the Linear() function here as it is unnecessary for a first pass at understanding autoencoders.\n         self.encoder = torch.nn.Sequential(\n            Linear(28 * 28, 128),\n            # ReLU(),\n            Linear(128, 64),\n            # ReLU(),\n            Linear(64, 32),\n            # ReLU(),\n            Linear(32, 2),\n         )\n\n         self.decoder = torch.nn.Sequential(\n            Linear(2, 32),\n            # ReLU(),\n            Linear(32, 64),\n            # ReLU(),\n            Linear(64, 128),\n            # ReLU(),\n            Linear(128, 28 * 28),\n            # Sigmoid(),\n         )\nThis architecture follows the same basic design as Figure 1 shows. The encoder part of the model (self.encoder) reduces the data’s dimension through these linear layers, little by little: 28*28 -&gt; 128 -&gt; 64 -&gt; 32 -&gt; 2. The compressed representation ends up being 2 pixels wide (2-dimensional) after the final Linear function Linear(32, 2).\nThe decoder (self.decoder) defines the same process as the encoder, but in reverse: linear layers go from smallest dimension (the middle layer) back to the original dimension of the image: 2 -&gt; 32 -&gt; 64 -&gt; 128 -&gt; 28*28. This is where the image reconstruction happens.\nHow is 2 pixels worth of information enough to reconstruct an image back to 784 pixels?\n\nConsider the obvious redundant information in the image - there’s a lot of empty space around the digits.\nDigits are built from similar shapes such as loops and lines. If the autoencoder knows the image it is trying to construct consists of two loops, it’s probably an 8. That already provides significant context for how the image will look.\nThe two values we are left with are not just pixel values like in the original image, they are interdependent. A change in the values in one dimension will affect the values in the other.\nThe reconstruction is lossy, so we are allowing for errors in the reconstruction to allow for a smaller size compressed image.\n\nMost importantly, the “prior experience” contains a lot of the information needed for the reconstruction. It is held in the model’s weights. The model weights define what is know as the latent space, these are the combination of functions (in our example, the Linear() functions) and their parameters that create the 2-dimensional space.\nCrucially, the “prior knowledge” encapsulated within the model’s weights carries a significant portion of the information necessary for the reconstruction process. These weights lay out the structure of the latent space. This space is a mesh of functions (for instance, the Linear() functions in our example make up much of this mesh) and their specific parameters to produce the two-dimensional representation."
  },
  {
    "objectID": "posts/2023-05-10-autoencodersv2.html#understanding-latent-spaces",
    "href": "posts/2023-05-10-autoencodersv2.html#understanding-latent-spaces",
    "title": "A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1",
    "section": "Understanding Latent spaces",
    "text": "Understanding Latent spaces\nIf it is possible to reconstruct the data from a lower dimensional representation, then the important information must still exist in that lower dimensional form. Something valuable must have been learned from the patterns in the data.\nThe term “latent” refers to something that is present but hidden. In this context, lower-dimensional representations already exist, it is just not obvious how to find them.\nConsider the plot of random points in Figure 8. To describe any single point, we need 3 pieces of information - the x coordinate, the y coordinate, and the z coordinate. If the data is random, there is no correlation between x, y, and z. In other words, changing x does not affect y or z and vice versa. We say the data spans a 3-dimensional space. There are no latent spaces.\n\n\nCode\nnum_points = 100\nx = 6 * np.random.randn(num_points)\ny = 6 * np.random.randn(num_points)\nz = 6 * np.random.randn(num_points)\n\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d', facecolor='white')\nax.scatter(x, y, z, c='r', marker='o', alpha=0.3, s=5)\n\nax.set_xlabel('X', labelpad=-10)\nax.set_ylabel('Y', labelpad=-10)\nax.set_zlabel('Z', labelpad=-10)\n\nax.grid(False)\n\nax.set_xlim(-10, 10)\nax.set_ylim(-10, 10)\nax.set_zlim(-10, 10)\n\nax.xaxis._axinfo['juggled'] = (0, 0, 0)\nax.yaxis._axinfo['juggled'] = (1, 1, 1)\nax.zaxis._axinfo['juggled'] = (2, 2, 2)\n\nax.set_xticks([])\nax.set_yticks([])\nax.set_zticks([])\n\nax.plot([-10, 10], [0, 0], [0, 0], color='k', linewidth=1)\nax.plot([0, 0], [-10, 10], [0, 0], color='k', linewidth=1)\nax.plot([0, 0], [0, 0], [-10, 10], color='k', linewidth=1)\n\nax.xaxis.pane.fill = False\nax.yaxis.pane.fill = False\nax.zaxis.pane.fill = False\n\nplt.show()\n\n\n\n\n\nFigure 8: 3D plot of random points\n\n\n\n\nNow let’s say x, y and z represent real world features: physical human traits.\n\nx = age\ny = height\nz = weight\n\nThe data would no longer be random. There are relationships between age,height, and weight. Additionally, some combinations of age, height and weight never occur in reality. You’ll be hard-pressed to find a 300 kg 4-year-old measuring 50 cm tall. So, if we have good sources of data, the model will also tend to believe such combinations are not possible. There are underlying physiological rules defining the relationship. Those rules define a latent space.\nConversely, given two of these values, we can make an approximate guess at the third. If we can do that, then there likely exists a smaller dimensional space in which our data also resides, one that does not require all the features. If done correctly, it might only take 1 or 2 coordinates to describe any human by these traits.\nThe goal is to discover a structure that serves as our new coordinate system, allowing navigation with just two coordinates. Autoencoders, particularly the encoder portion, accomplish this by seeking a lower-dimensional latent space. The autoencoder’s middle layer asserts the existence of a latent space of size \\(w\\) and compels the network to find such a representation. Lacking this constraint, the middle layer would remain in its original high-dimensional space, merely copying the input and functioning as an identity transformation (multiplying everything by 1).\nFigure 9 demonstrates that the majority of points lie on or near a 2D latent space. By projecting these points onto the blue grid as close to their original positions as possible, we can create a 2-dimensional visualization. What the axes represent is no longer as simple as height, weight or age, but some combination of all of them. Due to this projection, the points won’t precisely match their original locations, but they will be close. This process is analogous to the slightly blurred, imperfect image reconstruction from earlier on.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.decomposition import PCA\n\ndef generate_data(num_points):\n    age = np.random.randint(low=1, high=100, size=num_points)\n    height = np.zeros(num_points)\n    \n    for i, a in enumerate(age):\n        base_height = 50\n        growth_factor = 6\n        if a &lt;= 20:\n            growth_rate = a / 4.0\n            height[i] = base_height + a * growth_factor  + np.random.normal(loc=0, scale=20)\n        else:\n            max_height = base_height + 20 * growth_factor\n            height[i] = max_height + np.random.normal(loc=0, scale=5)\n    \n    weight = height * 0.5 + 1.2 * np.random.normal(loc=0, scale=10, size=num_points)\n    data = np.column_stack((age, height, weight))\n    return data\n\ndata = generate_data(500)\n\npca = PCA(n_components=2)\nlatent_space = pca.fit_transform(data)\n\ngrid_size = 30\nx_grid, y_grid = np.meshgrid(np.linspace(latent_space[:, 0].min(), latent_space[:, 0].max(), grid_size),\n                             np.linspace(latent_space[:, 1].min(), latent_space[:, 1].max(), grid_size))\n\nlatent_grid = np.column_stack((x_grid.ravel(), y_grid.ravel()))\n\nprojected_grid = pca.inverse_transform(latent_grid)\nage_grid, height_grid, weight_grid = projected_grid[:, 0].reshape(grid_size, grid_size), projected_grid[:, 1].reshape(grid_size, grid_size), projected_grid[:, 2].reshape(grid_size, grid_size)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(data[:,0], data[:,1], data[:,2], c='r', marker='o', alpha=0.3, s=5, label='Original Data')\nax.plot_surface(age_grid, height_grid, weight_grid, alpha=0.2, color='blue', linewidth=0, antialiased=False, label='Latent Space')\n\nax.set_xlabel('Age', labelpad=-5)\nax.set_ylabel('Height', labelpad=-5)\nax.set_zlabel('Weight', labelpad=-10)\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_zticklabels([])\nplt.show()\n\nfig, ax = plt.subplots(figsize=(6,5))\nax.scatter(-latent_space[:, 0], -latent_space[:, 1], c='r', marker='o', alpha=0.6, s=10)\n\nx_min, x_max = latent_space[:, 0].min(), latent_space[:, 0].max()\ny_min, y_max = latent_space[:, 1].min(), latent_space[:, 1].max()\nmargin = 0.1\nx_range = x_max - x_min\ny_range = y_max - y_min\n\nx_grid, y_grid = np.meshgrid(np.linspace(x_min - margin * x_range, x_max + margin * x_range, 30),\n                             np.linspace(y_min - margin * y_range, y_max + margin * y_range, 30))\n\nfor i in range(x_grid.shape[0]):\n    ax.plot(-x_grid[i, :], -y_grid[i, :], c='blue', alpha=0.2, linewidth=0.5)\n    ax.plot(-x_grid[:, i], -y_grid[:, i], c='blue', alpha=0.2, linewidth=0.5)\n\nax.set_xlabel('Latent Space Axis 1')\nax.set_ylabel('Latent Space Axis 2')\nax.set_xticklabels([])\nax.set_yticklabels([])\nplt.show()\n\n\n\n\n\n\n\n\n(a) Height vs Weight vs Age with Latent Space Overlay\n\n\n\n\n\n\n\n(b) Same data as (a) Projected onto Latent Space\n\n\n\n\nFigure 9: Visualisation of a Latent Space for height, weight and age in a population\n\n\n\nLatent spaces will not be so easy to visualise when it comes to complex data, and won’t be a flat plane. This is why we might want the autoencoder to figure out the shape for us."
  },
  {
    "objectID": "posts/2023-05-10-autoencodersv2.html#dimensionality-reduction-reveals-latent-spaces",
    "href": "posts/2023-05-10-autoencodersv2.html#dimensionality-reduction-reveals-latent-spaces",
    "title": "A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1",
    "section": "Dimensionality reduction reveals latent spaces",
    "text": "Dimensionality reduction reveals latent spaces\nWhen data is cleverly compressed, the reduced dimension space is a latent space, and autoencoders can find these latent spaces for us.\nConsider an image passed through our autoencoder model. We can interrupt the processing when it arrives at the middle layer, this allows us to capture the 2-dimensional latent representation of the image. In essence, each image becomes represented by just two coordinates within this latent space.\n\n\n\nFigure 10: Autoencoder Diagram with Decoder Removed\n\n\nBy processing multiple images through our autoencoder, we’re left with a set of 2D coordinates, each representing an individual image. Visualizing these coordinates within the latent space (as seen in Figure 11) unveils the underlying structure of our data. This process is akin to revealing the topography of an invisible landscape by sprinkling dust - each dust particle representing an image - and observing the surface that is created.\n\n\nCode\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.datasets as datasets\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nfrom sklearn.manifold import TSNE\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\ntrain_ds = torchvision.datasets.MNIST(\n    \"../data\",\n    train=True,\n    download=True,\n    transform=torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n        ]\n    ),\n)\n\ntest_ds = torchvision.datasets.MNIST(\n    \"../data\",\n    train=False,\n    download=True,\n    transform=torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n        ]\n    ),\n)\n\nbatch_size = 128\ntrain_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=False)\n\n\nclass AutoEncoder(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.encoder = torch.nn.Sequential(\n            torch.nn.Linear(28 * 28, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, 32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32, 2),\n        )\n\n        self.decoder = torch.nn.Sequential(\n            torch.nn.Linear(2, 32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 28 * 28),\n            torch.nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        enc = self.encoder(x)\n        dec = self.decoder(enc)\n        return dec\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = AutoEncoder()\nmodel_c = torch.compile(model)\nmodel_c.to(device)\nloss_f = torch.nn.MSELoss()\n\nopt = torch.optim.Adam(model.parameters(), lr=2e-3)\n\nepochs = 100\nout = []\nlosses = []\n\ntorch._dynamo.config.suppress_errors = True\n\n\ndef train_model():\n    model_c.train()\n\n    for epoch in tqdm(range(epochs)):\n        for img, _ in train_loader:\n            img = img.reshape(-1, 28 * 28).to(device)\n\n            reconstructed = model_c(img)\n            loss = loss_f(reconstructed, img)\n\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n            losses.append(loss.item())\n            print(f\"Loss: {loss.item()}\")\n        out.append((epoch, img, reconstructed))\n\n\nmodel_name = \"model_weights12-2dim.pt\"\nif Path(model_name).exists():\n    model.load_state_dict(torch.load(model_name))\nelse:\n    train_model()\n    torch.save(model.state_dict(), model_name)\n\nencoded_data = []\nlabels = []\noriginal_images = []\nc = 0\nfor i, (img, label) in tqdm(enumerate(test_loader)):\n    c += 1\n    img_flat = img.reshape(-1, 28 * 28)\n    out = model.encoder(img_flat)\n    encoded_data.extend(out.detach().cpu().numpy())\n    labels.extend(label.numpy())\n    original_images.extend(img_flat.cpu().numpy())\n    if c &gt; 4:\n        break\n\nfig, ax = plt.subplots()\n\nimage_size = 28\npadding = 0.03\n\nencoded_data = np.array(encoded_data)\n\nx_min, x_max = np.min(encoded_data[:, 0]), np.max(encoded_data[:, 0])\ny_min, y_max = np.min(encoded_data[:, 1]), np.max(encoded_data[:, 1])\nx_range = x_max - x_min\ny_range = y_max - y_min\n\nshift_x = padding / 2\nshift_y = padding / 2\nman_shift = 0.1 \n\nfor idx, (x, y) in enumerate(encoded_data):\n    x_pos = (x - x_min) / x_range - shift_x - man_shift\n    y_pos = (y - y_min) / y_range - shift_y\n    image = original_images[idx].reshape(28, 28)\n    ab = plt.Axes(fig, [x_pos, y_pos, padding, padding], frame_on=False)\n    ab.set_axis_off()\n    fig.add_axes(ab)\n    ab.imshow(image, cmap=\"gray_r\")\n\nax.set_xticks([])\nax.set_yticks([])\n\nplt.show()\n\n\n\n\n\nFigure 11: 2D Representation of latent dimension with original data projected onto it (source: Open code cell above for the code used to create this plot)\n\n\nA noteworthy observation is how similar digits cluster together in the visualized latent space. This demonstrates the model’s ability to discern patterns and similarities between different digits within its middle layer.\nAn autoencoders’ ability to learn essential features from the input data holds significant value. It is why we can compress the information. Without using an autoencoder, finding such latent spaces would be quite challenging.\nHopefully, the main concepts are now clear enough that we can start building a deeper, more technical understanding. In Part 2, we will delve more into implementation of autoencoder-based models to put the concepts discussed here with a more interesting example."
  },
  {
    "objectID": "posts/2023-05-10-autoencodersv2.html#revealing-latent-space-autoencoder",
    "href": "posts/2023-05-10-autoencodersv2.html#revealing-latent-space-autoencoder",
    "title": "A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1",
    "section": "Revealing latent space = autoencoder",
    "text": "Revealing latent space = autoencoder\nAn autoencoders’ ability to learn essential features from the input data holds significant value. This knowledge becomes highly beneficial when applied to more sophisticated and engaging machine learning challenges.\nIn Part 2, we will delve into implementation of autoencoder-based models to put the concepts discussed here into a full example."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel Obeng",
    "section": "",
    "text": "Welcome to danielobeng.com, the website of Daniel Obeng."
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Daniel Obeng",
    "section": "Blog Posts",
    "text": "Blog Posts"
  },
  {
    "objectID": "2023-05-05-VAE.html",
    "href": "2023-05-05-VAE.html",
    "title": "DO",
    "section": "",
    "text": "# show how autoencoder cant reproduce an input that is a merge of 2 numbers as one image?"
  },
  {
    "objectID": "2023-05-05-einsum.html",
    "href": "2023-05-05-einsum.html",
    "title": "DO",
    "section": "",
    "text": "omitting a letter in output means it will be summed this makes sense because after arrow defines the shape of the output, so if we want to reduce our input matrices such that they are outputted as the smaller size matrix, we’d have to sum up all the values in that dimension to create only 1 value eg 3x4 input shape has 12 values,if you summed all the values along rows you’d just be left with 4 values total"
  },
  {
    "objectID": "2023-05-05-bias_variance.html",
    "href": "2023-05-05-bias_variance.html",
    "title": "DO",
    "section": "",
    "text": "# explain bias variance but particlualr double descent cf notes and ilya suktveker notes"
  },
  {
    "objectID": "2023-04-17-autoencoders.html",
    "href": "2023-04-17-autoencoders.html",
    "title": "A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.lines import Line2D\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "2023-04-17-autoencoders.html#the-gist-of-an-autoencoder",
    "href": "2023-04-17-autoencoders.html#the-gist-of-an-autoencoder",
    "title": "A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1",
    "section": "The gist of an autoencoder",
    "text": "The gist of an autoencoder\nAutoencoders stand out as a unique class of neural network designed to compress input data and subsequently reconstruct it with high accuracy. This ingenious architecture consists of two interconnected neural networks working in tandem. Ultimately, connecting two neural networks together creates a single neural network, but it is useful to think of these as distinct parts when first learning how they work.\nThe two neural networks are called the encoder and the decoder and are more or less mirror images of each other (Figure 1). The encoder is responsible for compressing the input data into a more compact representation, while the decoder takes on the task of decompressing and reconstructing the compact representation back into the original form. Central to the success of this process is a layer of information nestled between the encoder and decoder. More on that later.\n\n\n\nFigure 1: Diagram of a basic Autoencoder (source: Wikipedia)\n\n\n\nFrom Images to Numerical Data: A Practical Example\nLet’s consider an analogy to illustrate the concept of compression, which is what teh encoder does. Imagine you are tasked with describing the car in Figure 2 to a friend, using only three words. Your friend must then draw and reproduce the car as accurately as possible based on your description.\n\n\n\nFigure 2: An orange lambo (source: Lamborghini)\n\n\nYou might choose the words “orange,” “fast,” and “angular”. In making these choices, you are drawing upon your experiences with other cars and how this particular car is different or similar to them. This allows you to generalize and create categories (also known as “features”) based on aspects such as color, speed, and shape.\nIf you were allowed more words, your description would likely be more accurate. With 1000 words, you could describe a very reasonable representation. This is true, but only up to a point.\nBut do you really need 1000 words to be accurate with your description? Could you obtain nearly identical results with only 500 words? Possibly, depending on whether any of the 1000 words were correlated or redundant. You might have the words “fast” and “200mph” in your list which essentially mean the same thing - it won’t help your friend draw the car any better having both.\nSo, how does this relate to autoencoders? Rather than words, we deal with numbers. In the context of images specifically, we deal with pixel values (a pixel colour is defined by 3 numbers for red, green and blue, e.g. pixel 1 = (100, 200, 90)), each of which can be though of as a feature. The goal of an autoencoder is to minimize redundancy these features. By doing so, it achieves compression, leaving only the most descriptive features that capture the essence of the input data.\n\n\nUnleashing the Power of the Middle Layer\nNow, let’s examine a practical example of how autoencoders compress and reconstruct data. Consider the original image in Figure 3, which we input into an autoencoder model. A compressed 15x2 representation is generated (as depicted in the middle column). This representation represents the middle layer of the model. Subsequently, the original image is reconstructed from this compressed form. Think about the journey of the image through the autoencoder in Figure 1 and when the transformations from original to middle column to reconstructed occur.\nIn this example, we have purposefully chosen a lower-quality reconstruction to emphasize that the process may not always be perfect, although it can achieve remarkable accuracy. It is indeed surprising how closely the reconstructed image resembles the original, despite using only a 15x2 representation as the starting point before decoding. At their core, images are simply numerical data on a computer, meaning this concept to be applied across various digital mediums, such as audio and text.\n\n\n\nFigure 3: Image Reconstruction Example (source: stackabuse.com)\n\n\nThe primary objective of an autoencoder extends beyond merely inputting an image and obtaining a reconstructed version. The true power of autoencoders lies in leveraging the information-rich middle layer in conjunction with a specific task, thereby enhancing the performance of machine learning models. Achieving accurate reconstruction of original data serves as an effective metric for monitoring how well autoencoder model has compressed information because the better it does this, the better the reconstruction. It helps in gauging the model’s ability to capture and represent the essential features of the input data.\n\n\nWhy are Autoencoders useful?\nThe true value of autoencoders lies in how concisely they compress information by capturing only the most defining features of the data. Humans can describe a 1000 pixel image of a car using just three words because our brains already have an algorithm that selects relevant categories as the “middle layer”. We aim to train autoencoders to achieve the same level of proficiency by forcing them to compress information, accomplished by feeding the model numerous images of cars until it learns the essential features.\nIdentifying these features unlocks many use cases, some of which include:\n\nData Compression: Utilize the encoder part of the autoencoder to reduce the size of an image file, enabling faster transfer speeds when shared with someone who possesses the corresponding decoder. Whether or not this is more useful than standard compression techniques really depends on the use case.\nNoise Reduction: Improve image quality by eliminating graininess, as the model learns that graininess is not an essential aspect of the image.\nGenerative Modeling: Generate new information, such as novel images or audio, using generative models. Recently, diffusion models have garnered significant attention in this area.\nAnomaly Detection: Identify unusual patterns or outliers in the data.\n\nMoreover, the architecture of autoencoders is well-suited for semi-supervised or unsupervised learning, which simplifies the data acquisition process. We will explore this topic in greater detail in a future post."
  },
  {
    "objectID": "2023-04-17-autoencoders.html#data-compression-retaining-the-core-information",
    "href": "2023-04-17-autoencoders.html#data-compression-retaining-the-core-information",
    "title": "A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1",
    "section": "Data compression: Retaining the Core Information",
    "text": "Data compression: Retaining the Core Information\nTo fully comprehend autoencoders, it is helpful to grasp the concept of data compression.\nIn the context of computers, compression does not literally mean storing the same number of bits of information in a smaller space, like you would compress a gas into a smaller volume of space. Instead, it serves as an analogy for removing bits that are not considered important, thereby reducing the file size—reminiscent of autoencoders. The bits we remove are those we can reproduce using an algorithm.\nAlthough we don’t compress data like we do the air in an oxygen tank, the end result is the same: we end up with something that takes up less space.\nLet’s consider a simple example to illustrate this point. Imagine a 4x4 grid with four red squares in the center, surrounded by blue squares. Each square represents one bit of memory, totaling 16 bits of information. Our goal is to create an algorithm capable of compressing the size of this grid. We also know the following facts about blue and red squares:\n\nRed squares always appear in 2x2 grids.\nBlue squares consistently surround red square groups.\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ngrid = np.ones((4, 4))\ngrid[1:3, 1:3] = 0\n\ncmap = plt.cm.get_cmap('coolwarm_r', 2)\n\nfig, ax = plt.subplots(figsize=(2, 2))\nax.imshow(grid, cmap=cmap, vmin=0, vmax=1)\n\nax.set_xticks(np.arange(0.5, 3.5, 1), minor=True)\nax.set_yticks(np.arange(0.5, 3.5, 1), minor=True)\n\nax.grid(which='minor', color='black', linewidth=2)\n\nax.set_xticks([])\nax.set_yticks([])\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nWith this knowledge, we can develop an algorithm to compress the information in the image above. We can represent the original image using just one red square and one blue square side by side, as we know that a red square must be accompanied by three more red squares in a 2x2 formation. If a blue square is adjacent to the red square, it must surround the red squares. We have effectively compressed the information from 16 bits to 2 bits, retaining only the core essence of this blue and red square world.\n\n\nCode\nfaint_red = (0.705673158, 0.01555616, 0.150232812, 0.3)\nfaint_blue = (0.2298057, 0.298717966, 0.753683153, 0.3)\nred = (0.705673158, 0.01555616, 0.150232812)\nblue = (0.2298057, 0.298717966, 0.753683153)\n\ncustom_cmap = ListedColormap([faint_red, faint_blue, red, blue])\n\ngrid = np.zeros((4, 4))\ngrid[0, ::1] = 1\ngrid[3, ::1] = 1\ngrid[:, 0] = 1\ngrid[:, 3] = 1\n\ngrid[2, 2] = 2\ngrid[2, 3] = 3\n\nfig, ax = plt.subplots(figsize=(2, 2))\nax.imshow(grid, cmap=custom_cmap, vmin=0, vmax=3)\nax.set_xticks(np.arange(0.5, 3.5, 1), minor=True)\nax.set_yticks(np.arange(0.5, 3.5, 1), minor=True)\n\nax.grid(which='minor', color='lightgray', linewidth=2)\n\nax.add_line(Line2D([1.5, 1.5], [1.5, 2.5], color='black', linewidth=2))\nax.add_line(Line2D([2.5, 2.5], [1.5, 2.5], color='black', linewidth=2))\nax.add_line(Line2D([3.5, 3.5], [1.5, 2.5], color='black', linewidth=2))\nax.add_line(Line2D([1.5, 3.5], [1.5, 1.5], color='black', linewidth=2))\nax.add_line(Line2D([1.5, 3.5], [2.5, 2.5], color='black', linewidth=2))\nax.set_xticks([])\nax.set_yticks([])\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nIt is worth noting the role of the algorithm in this process. To compress and decompress the data, the algorithm must possess an adequate understanding of this tiny world. In our example, we designed the algorithm with pre-existing knowledge.\nIn most real-world scenarios, there is significantly more data involved, and the relationships are not as easily discernible, leading to more complex algorithms with more intricate pre-existing knowledge.\n\nMore context on compression\nWhen dealing with more complex data, such as an image containing a greater number of bits, the design of the algorithm must be more sophisticated. However, the main principle remains the same: devise ways to represent a certain number of bits with fewer bits, which necessitates understanding the essential relationships between those bits of information.\nNot all compression algorithms perform equally. Some algorithms allow you to perfectly replicate the original information, referred to as “lossless” compression. Conversely, when the information cannot be replicated flawlessly, it is called “lossy” compression. The jpeg image format, for instance, employs a lossy algorithm. jpeg files are small due to an algorithm working behind the scenes. In general, lossy algorithms can compress more data than lossless ones.\nYour choice between the two depends on your specific needs. For an image, a lossy compression may suffice if you only need it to be clear enough to read a document. However, an audiophile seeking a perfect reproduction of their favorite song would likely opt for lossless compression.\n\n\nBack to Autoencoders\nWhat sets autoencoders apart from general compression algorithms?\n\nThe algorithm is not designed by humans: autoencoders are neural networks, and the compression and decompression algorithm is developed by the network itself. While humans do have some input in determining the size of the middle layer, for example, the network must figure out how to make it work. This is where the “auto” in “autoencoder” comes from.\nThe algorithm is task-specific: autoencoders are trained on specific data and learn to compress data that is similar in nature. They can excel at compressing particular types of data, but they do not generalize beyond the data provided. If they are only fed images of cars, they won’t compress images of flowers effectively. However, if given both images of cars and flowers, they could handle both. For successful compression, the network must have learned something valuable about the data’s world and stored that knowledge in the middle layer of the network. That knowledge lies in what is known as the latent space.\n\nThe term “latent” refers to something that is present but hidden. In this context, compressed representations consisting of fewer dimensions already exist, it is just not obvious from the point of view of the higher dimensional space how to find them."
  },
  {
    "objectID": "2023-04-17-autoencoders.html#understanding-latent-spaces",
    "href": "2023-04-17-autoencoders.html#understanding-latent-spaces",
    "title": "A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1",
    "section": "Understanding Latent Spaces",
    "text": "Understanding Latent Spaces\nConsider the plot of random points in Figure 4. To describe any single point, we need 3 pieces of information - the x coordinate, the y coordinate, and the z coordinate. If the data is random, there is no correlation between x, y, and z. In other words, changing x does not affect y or z and vice versa. We say the data spans a 3-dimensional space. There are no latent spaces.\n\n\nCode\nnum_points = 100\nx = 6 * np.random.randn(num_points)\ny = 6 * np.random.randn(num_points)\nz = 6 * np.random.randn(num_points)\n\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d', facecolor='white')\nax.scatter(x, y, z, c='r', marker='o', alpha=0.3, s=5)\n\nax.set_xlabel('X', labelpad=-10)\nax.set_ylabel('Y', labelpad=-10)\nax.set_zlabel('Z', labelpad=-10)\n\nax.grid(False)\n\nax.set_xlim(-10, 10)\nax.set_ylim(-10, 10)\nax.set_zlim(-10, 10)\n\nax.xaxis._axinfo['juggled'] = (0, 0, 0)\nax.yaxis._axinfo['juggled'] = (1, 1, 1)\nax.zaxis._axinfo['juggled'] = (2, 2, 2)\n\nax.set_xticks([])\nax.set_yticks([])\nax.set_zticks([])\n\nax.plot([-10, 10], [0, 0], [0, 0], color='k', linewidth=1)\nax.plot([0, 0], [-10, 10], [0, 0], color='k', linewidth=1)\nax.plot([0, 0], [0, 0], [-10, 10], color='k', linewidth=1)\n\nax.xaxis.pane.fill = False\nax.yaxis.pane.fill = False\nax.zaxis.pane.fill = False\n\nplt.show()\n\n\n\n\n\nFigure 4: 3D plot of random points\n\n\n\n\nNow let’s say x, y and z represent real world features: physical human traits.\n\nx = age\ny = height\nz = weight\n\nThe data would no longer be random. There are relationships between age,height, and weight. Additionally, some combinations of age, height and weight never occur in reality. You’ll be hard-pressed to find a 300 kg 4-year-old measuring 50 cm tall. So, if we have good sources of data, the model will also tend to believe such combinations are not possible. There are underlying physiological rules defining the relationship. Those rules define a latent space.\nConversely, given two of these values, we can make an approximate guess at the third. If we can do that, then there likely exists a smaller dimensional space in which our data also resides, one that does not require all the features. If done correctly, it might only take 1 or 2 coordinates to describe any human by these traits.\nThe goal is to discover a structure that serves as our new coordinate system, allowing navigation with just two coordinates. Autoencoders, particularly the encoder portion, accomplish this by seeking a lower-dimensional latent space. The autoencoder’s middle layer asserts the existence of a latent space of size \\(w\\) and compels the network to find such a representation. Lacking this constraint, the middle layer would remain in its original high-dimensional space, merely copying the input and functioning as an identity transformation (multiplying everything by 1).\nFigure 5 demonstrates that the majority of points lie on or near a 2D latent space. By projecting these points onto the blue grid as close to their original positions as possible, we can create a 2-dimensional visualization. What the axes represent is no longer as simple as height, weight or age, but some combination of all of them. Due to this projection, the points won’t precisely match their original locations, but they will be close. This process is analogous to the slightly blurred, imperfect image reconstruction from earlier on.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.decomposition import PCA\n\ndef generate_data(num_points):\n    age = np.random.randint(low=1, high=100, size=num_points)\n    height = np.zeros(num_points)\n    \n    for i, a in enumerate(age):\n        base_height = 50\n        growth_factor = 6\n        if a &lt;= 20:\n            growth_rate = a / 4.0\n            height[i] = base_height + a * growth_factor  + np.random.normal(loc=0, scale=20)\n        else:\n            max_height = base_height + 20 * growth_factor\n            height[i] = max_height + np.random.normal(loc=0, scale=5)\n    \n    weight = height * 0.5 + 1.2 * np.random.normal(loc=0, scale=10, size=num_points)\n    data = np.column_stack((age, height, weight))\n    return data\n\ndata = generate_data(500)\n\npca = PCA(n_components=2)\nlatent_space = pca.fit_transform(data)\n\ngrid_size = 30\nx_grid, y_grid = np.meshgrid(np.linspace(latent_space[:, 0].min(), latent_space[:, 0].max(), grid_size),\n                             np.linspace(latent_space[:, 1].min(), latent_space[:, 1].max(), grid_size))\n\nlatent_grid = np.column_stack((x_grid.ravel(), y_grid.ravel()))\n\nprojected_grid = pca.inverse_transform(latent_grid)\nage_grid, height_grid, weight_grid = projected_grid[:, 0].reshape(grid_size, grid_size), projected_grid[:, 1].reshape(grid_size, grid_size), projected_grid[:, 2].reshape(grid_size, grid_size)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(data[:,0], data[:,1], data[:,2], c='r', marker='o', alpha=0.3, s=5, label='Original Data')\nax.plot_surface(age_grid, height_grid, weight_grid, alpha=0.2, color='blue', linewidth=0, antialiased=False, label='Latent Space')\n\nax.set_xlabel('Age', labelpad=-5)\nax.set_ylabel('Height', labelpad=-5)\nax.set_zlabel('Weight', labelpad=-10)\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_zticklabels([])\nplt.show()\n\nfig, ax = plt.subplots(figsize=(6,5))\nax.scatter(-latent_space[:, 0], -latent_space[:, 1], c='r', marker='o', alpha=0.6, s=10)\n\nx_min, x_max = latent_space[:, 0].min(), latent_space[:, 0].max()\ny_min, y_max = latent_space[:, 1].min(), latent_space[:, 1].max()\nmargin = 0.1\nx_range = x_max - x_min\ny_range = y_max - y_min\n\nx_grid, y_grid = np.meshgrid(np.linspace(x_min - margin * x_range, x_max + margin * x_range, 30),\n                             np.linspace(y_min - margin * y_range, y_max + margin * y_range, 30))\n\nfor i in range(x_grid.shape[0]):\n    ax.plot(-x_grid[i, :], -y_grid[i, :], c='blue', alpha=0.2, linewidth=0.5)\n    ax.plot(-x_grid[:, i], -y_grid[:, i], c='blue', alpha=0.2, linewidth=0.5)\n\nax.set_xlabel('Latent Space Axis 1')\nax.set_ylabel('Latent Space Axis 2')\nax.set_xticklabels([])\nax.set_yticklabels([])\nplt.show()\n\n\n\n\n\n\n\n\n(a) Height vs Weight vs Age with Latent Space Overlay\n\n\n\n\n\n\n\n(b) Same data as (a) Projected onto Latent Space\n\n\n\n\nFigure 5: Visualisation of a Latent Space for height, weight and age in a population\n\n\n\nLatent spaces will not be so easy to visualise when it comes to complex data, and won’t be a flat plane. This is why we want the Autoencoder to figure out the shape for us."
  },
  {
    "objectID": "2023-04-17-autoencoders.html#autoencoder-for-compression-example---mnist",
    "href": "2023-04-17-autoencoders.html#autoencoder-for-compression-example---mnist",
    "title": "A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1",
    "section": "Autoencoder for compression example - MNIST",
    "text": "Autoencoder for compression example - MNIST\nIn this section, we will cover a high level the fundamental concepts of how Autoencoders compress and reconstruct data using the MNIST dataset as an example. A more in-depth training guide will be provided in a future post.\nThe MNIST dataset (Figure 6) consists of 28x28 pixel images of handwritten digits from 0 to 9, resulting in 28x28=784 features (each pixel is a feature) to describe a digit.\nWe will explore the following aspects of the model:\n\nConstructing the autoencoder model - an overview of the compression process.\nThe autoencoder architechture - how the model reproduces the input by passing it through the input, middle layer, and output.\nSlicing the model by removing the decoder section, which allows us to obtain the latent space coordinates of an input image instead of its reconstruction.\nVisualizing the position of images in the latent space\n\n\n\n\nFigure 6: Images from the MNIST dataset\n\n\nExamining the code that defines the autoencoder’s architecture using the MNIST data helps us understand how it reduces the dimensionality of the data. The code below uses the Pytorch library to define the model layers.\nThe important lines of code involve the Linear() function (a linear layer), which essentially takes us from a dimension of size \\(a\\) to a dimension of size \\(b\\) via Linear(a,b).\nThe encoder part of the model (self.encoder) reduces the data’s dimension through these linear layers, little by little: 28*28 -&gt; 128 -&gt; 64 -&gt; 32 -&gt; 10.\nThe decoder (self.decoder) is the same process, but in reverse: linear layers go from smallest dimension (the middle layer, consiting of 10 dimensions) to the original dimension of the image: 10 -&gt; 32 -&gt; 64 -&gt; 128 -&gt; 28*28. This is where the reconstruction happens.\n         self.encoder = torch.nn.Sequential(\n            Linear(28 * 28, 128),\n            # ReLU(),\n            Linear(128, 64),\n            # ReLU(),\n            Linear(64, 32),\n            # ReLU(),\n            Linear(32, 10),\n         )\n\n         self.decoder = torch.nn.Sequential(\n            Linear(10, 32),\n            # ReLU(),\n            Linear(32, 64),\n            # ReLU(),\n            Linear(64, 128),\n            # ReLU(),\n            Linear(128, 28 * 28),\n            # Sigmoid(),\n         )\nThis very autoencoder architecture was used to create the reconstruction in Figure 7. Again, the reconstructed image is intentionally blurred to better illustrate the concept of reconstruction. In practice, with such small images, an autoencoder can achieve reconstructions that more closely resemble the original input.\n\n\n\nFigure 7: Autoencoder example\n\n\n\n\n\nFigure 8: Input Image and Decoder Output\n\n\nWhen an image is processed through this model, it generates a reconstructed output. However, if we halt the process at the middle layer, we obtain the 10-dimensional latent space representation, which consists of a set of 10 coordinates.\n\n\n\nFigure 9: Autoencoder Diagram with Decoder Removed\n\n\n\n\n\nFigure 10: Output from Autoencoder Middle Layer\n\n\nMoreover, we can visualise the latent space by plotting the positions of multiple images that were passed through the encoder layer only in (Figure 11). In order to interpret what a 10-dimensional space looks like, we need to approximate it by further flattening it down to 2 dimensions using a technique called t-SNE (how this works is not important to understand).\nObserve how similar-looking digits cluster together, signifying that the model’s middle layer can discern the resemblance between different digits. Most notably, we can now characterize a particular digit, such as “1,” using only two coordinates instead of the original 784 features, highlighting the power of autoencoders for dimensionality reduction.\n\n\nCode\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nfrom sklearn.manifold import TSNE\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom tqdm import tqdm\n\ntrain_ds = torchvision.datasets.MNIST(\n    \"../data\",\n    train=True,\n    download=True,\n    transform=torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n        ]\n    ),\n)\n\ntest_ds = torchvision.datasets.MNIST(\n    \"../data\",\n    train=False,\n    download=True,\n    transform=torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n        ]\n    ),\n)\n\nepochs = 20\nbatch_size = 128\ntrain_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=False)\n\n\nclass AutoEncoder(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.encoder = torch.nn.Sequential(\n            torch.nn.Linear(28 * 28, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, 32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32, 10),\n        )\n\n        self.decoder = torch.nn.Sequential(\n            torch.nn.Linear(10, 32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 28 * 28),\n            torch.nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        enc = self.encoder(x)\n        dec = self.decoder(enc)\n        return dec\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = AutoEncoder()\n# model = torch.compile(model) #if using torch 2.0\n\nmodel.to(device)\nloss_f = torch.nn.MSELoss()\n\nopt = torch.optim.Adam(model.parameters(), lr=2e-3)\n\nout = []\nlosses = []\n\n\ndef train_model():\n    model.train()\n\n    for epoch in tqdm(range(epochs)):\n        for img, _ in train_loader:\n            img = img.reshape(-1, 28 * 28).to(device)\n\n            reconstructed = model(img)\n            loss = loss_f(reconstructed, img)\n\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n            losses.append(loss.item())\n            print(f\"Loss: {loss.item()}\")\n        out.append((epoch, img, reconstructed))\n\n\nmodel_name = \"model_weights.pt\"\nif Path(model_name).exists():\n    model.load_state_dict(torch.load(model_name))\nelse:\n    train_model()\n    torch.save(model.state_dict(), model_name)\n\ntest_image, _ = test_ds[4]\n\nencoded_data = []\noriginal_images = []\nc = 0\nfor i, (img, label) in tqdm(enumerate(test_loader)):\n    c += 1\n    img_flat = img.reshape(-1, 28 * 28)\n    out = model.encoder(img_flat)\n    encoded_data.extend(out.detach().cpu().numpy())\n    original_images.extend(img_flat.cpu().numpy())\n    if c &gt; 3:\n        break\n\ntsne = TSNE(n_components=2, random_state=42)\ntsne_output = tsne.fit_transform(np.array(encoded_data))\n\nfig, ax = plt.subplots()\n\nimage_size = 28\npadding = 0.06\npadding_in_pixels = int(padding * image_size)\n\ntsne_min, tsne_max = np.min(tsne_output, axis=0), np.max(tsne_output, axis=0)\ntsne_range = tsne_max - tsne_min\nnormalized_output = (tsne_output - tsne_min) / tsne_range\n\nfor idx, (x, y) in tqdm(enumerate(normalized_output)):\n    x_pos = x * (1 - 2 * padding) + padding\n    y_pos = y * (1 - 2 * padding) + padding\n    image = original_images[idx].reshape(28, 28)\n    im = np.array(image, dtype=np.float32)\n    ab = plt.Axes(fig, [x_pos, y_pos, padding, padding])\n    ab.set_axis_off()\n    fig.add_axes(ab)\n    ab.imshow(im, cmap=\"gray_r\")\n\nax.set_xticks([])\nax.set_yticks([])\n\nplt.show()\n\n\n\n\n\nFigure 11: 2D Representation of latent dimension (source: Open code cell above for the code used to create this plot)\n\n\nAn autoencoders’ ability to learn essential features from the input data holds significant value. This knowledge becomes highly beneficial when applied to more sophisticated and engaging machine learning challenges.\nIn Part 2, we will delve into implementation of autoencoder-based models to put the concepts discussed here into a full example."
  },
  {
    "objectID": "posts/2023-05-10-autoencodersv2.html#an-autoencoder-example",
    "href": "posts/2023-05-10-autoencodersv2.html#an-autoencoder-example",
    "title": "A Comprehensive Guide to the Intuition, Mathematics, and Implementation of Autoencoders - Part 1",
    "section": "An Autoencoder Example",
    "text": "An Autoencoder Example\nThroughout this blog post, we will use a basic autoencoder model as an example. The goal is to use this example throughout the blog post as an anchor to which we can attach new concepts as we learn them.\nThis particular autoencoder can compresses and reconstruct images from the MNIST dataset (Figure 3) which consists of 28x28 pixel, black and white images of handwritten digits from 0 to 9. The pixel values range from 0-1 where 1=white, 0=black (though it doesn’t matter which way round you represent the colours) and any shade of grey will be a value between the two.\n\n\n\nFigure 3: Images from the MNIST dataset\n\n\nFigure 4 shows a handwritten digit that was reconstructed by the autoencoder model we will be using. The reconstruction is (purposefully) not perfect to highlight that it is a reconstruction.\n\n\n\nFigure 4: MNIST image reconstruction\n\n\nNow that we’ve set up our example, let’s explore the first concept: data compression."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Daniel Obeng. I’m a machine learning engineer with an Msc. in Engineering and a Masters in IP Law."
  }
]